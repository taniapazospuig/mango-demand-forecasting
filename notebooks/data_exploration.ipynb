{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9d631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('data/train.csv', sep=\";\", header=0)\n",
    "df_test = pd.read_csv('data/test.csv', sep=\";\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5638552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "cols_to_remove = [\"heel_shape_type\", \"toecap_type\", \"archetype\"]\n",
    "df.drop(columns=cols_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2817821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if waist_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null waist_type value\n",
    "family_waist_applicable = df.groupby(\"family\")[\"waist_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"waist_applicable\"] = df[\"family\"].map(family_waist_applicable).astype(int)\n",
    "\n",
    "def clean_waist_type(row):\n",
    "    wt = row[\"waist_type\"]\n",
    "    applicable = row[\"waist_applicable\"]\n",
    "\n",
    "    if pd.notnull(wt):\n",
    "        return wt                           # Real waist type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"          # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"         # Attribute irrelevant for this category\n",
    "\n",
    "df[\"waist_type\"] = df.apply(clean_waist_type, axis=1)\n",
    "df.drop(columns=[\"waist_applicable\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581ec43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if length_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null length_type value\n",
    "family_length_applicable = df.groupby(\"family\")[\"length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"length_applicable\"] = df[\"family\"].map(family_length_applicable).astype(int)\n",
    "\n",
    "def clean_length_type(row):\n",
    "    lt = row[\"length_type\"]\n",
    "    applicable = row[\"length_applicable\"]\n",
    "\n",
    "    if pd.notnull(lt):\n",
    "        return lt                           # Real length type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"          # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"         # Attribute irrelevant for this category\n",
    "\n",
    "df[\"length_type\"] = df.apply(clean_length_type, axis=1)\n",
    "df.drop(columns=[\"length_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308894cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if silhouette_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null silhouette_type value\n",
    "family_silhouette_applicable = df.groupby(\"family\")[\"silhouette_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"silhouette_applicable\"] = df[\"family\"].map(family_silhouette_applicable).astype(int)\n",
    "\n",
    "def clean_silhouette_type(row):\n",
    "    st = row[\"silhouette_type\"]\n",
    "    applicable = row[\"silhouette_applicable\"]\n",
    "\n",
    "    if pd.notnull(st):\n",
    "        return st                          # Real silhouette type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"         # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"        # Attribute irrelevant for this category\n",
    "\n",
    "df[\"silhouette_type\"] = df.apply(clean_silhouette_type, axis=1)\n",
    "df.drop(columns=[\"silhouette_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329e3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if neck_lapel_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null neck_lapel_type value\n",
    "family_neck_lapel_applicable = df.groupby(\"family\")[\"neck_lapel_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"neck_lapel_applicable\"] = df[\"family\"].map(family_neck_lapel_applicable).astype(int)\n",
    "\n",
    "def clean_neck_lapel_type(row):\n",
    "    nlt = row[\"neck_lapel_type\"]\n",
    "    applicable = row[\"neck_lapel_applicable\"]\n",
    "\n",
    "    if pd.notnull(nlt):\n",
    "        return nlt                         # Real neck/lapel type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"         # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"        # Attribute irrelevant for this category\n",
    "\n",
    "df[\"neck_lapel_type\"] = df.apply(clean_neck_lapel_type, axis=1)\n",
    "df.drop(columns=[\"neck_lapel_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c81c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if sleeve_length_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null sleeve_length_type value\n",
    "family_sleeve_length_applicable = df.groupby(\"family\")[\"sleeve_length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"sleeve_length_applicable\"] = df[\"family\"].map(family_sleeve_length_applicable).astype(int)\n",
    "\n",
    "def clean_sleeve_length_type(row):\n",
    "    slt = row[\"sleeve_length_type\"]\n",
    "    applicable = row[\"sleeve_length_applicable\"]\n",
    "\n",
    "    if pd.notnull(slt):\n",
    "        return slt                         # Real sleeve length type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"         # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"        # Attribute irrelevant for this category\n",
    "\n",
    "df[\"sleeve_length_type\"] = df.apply(clean_sleeve_length_type, axis=1)\n",
    "df.drop(columns=[\"sleeve_length_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c35aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if woven_structure is applicable to each family\n",
    "# A family is applicable if it has at least one non-null woven_structure value\n",
    "family_woven_structure_applicable = df.groupby(\"family\")[\"woven_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"woven_structure_applicable\"] = df[\"family\"].map(family_woven_structure_applicable).astype(int)\n",
    "\n",
    "def clean_woven_structure(row):\n",
    "    ws = row[\"woven_structure\"]\n",
    "    applicable = row[\"woven_structure_applicable\"]\n",
    "\n",
    "    if pd.notnull(ws):\n",
    "        return ws                         # Real woven structure from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"        # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"       # Attribute irrelevant for this category\n",
    "\n",
    "df[\"woven_structure\"] = df.apply(clean_woven_structure, axis=1)\n",
    "df.drop(columns=[\"woven_structure_applicable\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739804d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if knit_structure is applicable to each family\n",
    "# A family is applicable if it has at least one non-null knit_structure value\n",
    "family_knit_structure_applicable = df.groupby(\"family\")[\"knit_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"knit_structure_applicable\"] = df[\"family\"].map(family_knit_structure_applicable).astype(int)\n",
    "\n",
    "def clean_knit_structure(row):\n",
    "    ks = row[\"knit_structure\"]\n",
    "    applicable = row[\"knit_structure_applicable\"]\n",
    "\n",
    "    if pd.notnull(ks):\n",
    "        return ks                         # Real knit structure from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"       # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"      # Attribute irrelevant for this category\n",
    "\n",
    "df[\"knit_structure\"] = df.apply(clean_knit_structure, axis=1)\n",
    "df.drop(columns=[\"knit_structure_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af503099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create is_fall boolean column based on id_season\n",
    "# Pattern: even id_season values are fall (1), odd values are not fall (0)\n",
    "# Examples: 89->0, 88->1, 87->0, 86->1\n",
    "df['is_fall'] = (df['id_season'] % 2 == 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87fce06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all missing values in print_type with \"Sin Estampado\"\n",
    "df[\"print_type\"] = df[\"print_type\"].fillna(\"Sin Estampado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a61e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weeks_since_launch column\n",
    "# Group by ID and id_season, then rank by num_week_iso (0 = launch week, 1 = second week, etc.)\n",
    "df = df.sort_values(['ID', 'id_season', 'year', 'num_week_iso'])\n",
    "\n",
    "# Works because the difference in num_week_iso is always 1 (same year) or 51 (different year)\n",
    "# Create weeks_since_launch: rank within each (ID, id_season) group\n",
    "# The smallest num_week_iso gets 0, next gets 1, etc.\n",
    "df['weeks_since_launch'] = (df.groupby(['ID', 'id_season'])['num_week_iso'].rank(method='dense', ascending=True) - 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031206d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Fix for Windows threadpoolctl/OpenBLAS issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Disable threadpoolctl to avoid OpenBLAS inspection error on Windows\n",
    "try:\n",
    "    from sklearn import config_context\n",
    "    # Use config_context to disable threadpoolctl\n",
    "    import sklearn.utils.fixes\n",
    "    # Monkey patch threadpool_limits to be a no-op\n",
    "    original_threadpool_limits = sklearn.utils.fixes.threadpool_limits\n",
    "    \n",
    "    class NoOpContext:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            return False\n",
    "    \n",
    "    def patched_threadpool_limits(*args, **kwargs):\n",
    "        return NoOpContext()\n",
    "    \n",
    "    sklearn.utils.fixes.threadpool_limits = patched_threadpool_limits\n",
    "except Exception:\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "# Color features - using color_name with k-means clustering instead of RGB\n",
    "# We'll cluster color names using label encoding and k-means (k=15)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Store color_name before dropping (we'll use it for clustering)\n",
    "color_names_train = df[\"color_name\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color name clustering with k-means (k=15)\n",
    "# Label encode color names for k-means\n",
    "print(\"Creating color clusters from color_name...\")\n",
    "le_color = LabelEncoder()\n",
    "color_encoded = le_color.fit_transform(color_names_train.fillna(\"UNKNOWN\"))\n",
    "\n",
    "# Reshape for k-means (1D array needs to be 2D)\n",
    "color_encoded_2d = color_encoded.reshape(-1, 1)\n",
    "\n",
    "# Apply k-means clustering on encoded color names\n",
    "n_color_clusters = 15\n",
    "kmeans_color = KMeans(n_clusters=n_color_clusters, n_init=10, random_state=42)\n",
    "color_clusters = kmeans_color.fit_predict(color_encoded_2d)\n",
    "\n",
    "# Add color cluster to dataframe\n",
    "df[\"color_cluster\"] = color_clusters.astype(int)\n",
    "\n",
    "# Calculate distance to color cluster centroid\n",
    "color_dists = kmeans_color.transform(color_encoded_2d)\n",
    "df[\"color_cluster_dist\"] = color_dists.min(axis=1)\n",
    "\n",
    "print(f\"Created {n_color_clusters} color clusters from color names\")\n",
    "print(f\"Color clusters: {df['color_cluster'].nunique()} unique clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop RGB-related columns (R, G, B, color_rgb) - we're using color_name clustering instead\n",
    "cols_to_drop = [\"color_rgb\"]\n",
    "for col in [\"R\", \"G\", \"B\"]:\n",
    "    if col in df.columns:\n",
    "        cols_to_drop.append(col)\n",
    "if len(cols_to_drop) > 1:\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Dropped RGB columns: {cols_to_drop}\")\n",
    "else:\n",
    "    df.drop(columns=[\"color_rgb\"], inplace=True)\n",
    "    print(\"Dropped color_rgb column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812956c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all negative values for weekly_sales and weekly_demand to 0\n",
    "df.loc[df[\"weekly_sales\"] < 0, \"weekly_sales\"] = 0\n",
    "df.loc[df[\"weekly_demand\"] < 0, \"weekly_demand\"] = 0\n",
    "\n",
    "# Add seasonality features: week 23 and Black Friday\n",
    "# Black Friday is typically around week 47-48 in ISO week numbering (late November)\n",
    "# Week 23 is typically around late May/early June (end of spring/start of summer sales)\n",
    "print(\"\\nAdding seasonality features...\")\n",
    "\n",
    "# Week 23 indicator (specific shopping period)\n",
    "df[\"is_week_23\"] = (df[\"num_week_iso\"] == 23).astype(int)\n",
    "\n",
    "# Black Friday indicator (typically weeks 47-48, we'll use 47 as it's more common)\n",
    "# Also check if it's around late November (week 47-48 in most years)\n",
    "df[\"is_black_friday\"] = (df[\"num_week_iso\"].isin([47, 48])).astype(int)\n",
    "\n",
    "print(f\"Week 23 occurrences: {df['is_week_23'].sum()}\")\n",
    "print(f\"Black Friday occurrences: {df['is_black_friday'].sum()}\")\n",
    "\n",
    "# Remove low importance features based on feature_importance.csv\n",
    "# Low importance features to remove:\n",
    "low_importance_features = [\n",
    "    \"cluster_velocity_1_6\",      # importance: 125204640.0\n",
    "    \"cluster_peak_week\",          # importance: 1397215083.0\n",
    "    \"family_demand_trend\",        # importance: 1442911771.0\n",
    "    \"cluster_popularity\",         # importance: 1717375991.0\n",
    "    \"cluster_demand_slope\",       # importance: 2034475197.0\n",
    "    \"cluster_season_growth\",      # importance: 2698429847.0\n",
    "    \"aggregated_family\",          # importance: 3451807769.0 (if exists)\n",
    "    \"cluster_yoy_change\",         # importance: 3807712455.0\n",
    "]\n",
    "\n",
    "# Remove these features if they exist (they may be created later in the notebook)\n",
    "features_to_remove = [f for f in low_importance_features if f in df.columns]\n",
    "if features_to_remove:\n",
    "    df.drop(columns=features_to_remove, inplace=True)\n",
    "    print(f\"\\nRemoved low importance features: {features_to_remove}\")\n",
    "else:\n",
    "    print(\"\\nLow importance features will be removed after they are created (later in notebook)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4ac9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save will be done at the end of the notebook\n",
    "# df.to_csv('train_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb558804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING CODE - COMMENTED OUT\n",
    "# # Determine optimal number of clusters using multiple methods\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Range of k values to test\n",
    "# k_range = range(2, 21)  # Test k from 2 to 20\n",
    "# inertias = []\n",
    "# silhouette_scores = []\n",
    "# \n",
    "# print(\"Evaluating different numbers of clusters...\")\n",
    "# for k in k_range:\n",
    "#     kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "#     labels = kmeans_temp.fit_predict(rgb_scaled)\n",
    "#     \n",
    "#     inertias.append(kmeans_temp.inertia_)\n",
    "#     silhouette_scores.append(silhouette_score(rgb_scaled, labels))\n",
    "#     \n",
    "#     if k % 5 == 0:\n",
    "#         print(f\"  Completed k={k}...\")\n",
    "# \n",
    "# # Find optimal k based on silhouette score\n",
    "# optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "# \n",
    "# print(f\"\\nOptimal k based on Silhouette Score: {optimal_k_silhouette} (score: {max(silhouette_scores):.3f})\")\n",
    "# \n",
    "# # Plot the results\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# # Elbow Method (Inertia)\n",
    "# axes[0].plot(k_range, inertias, 'bo-')\n",
    "# axes[0].set_xlabel('Number of Clusters (k)')\n",
    "# axes[0].set_ylabel('Within-Cluster Sum of Squares (Inertia)')\n",
    "# axes[0].set_title('Elbow Method')\n",
    "# axes[0].grid(True)\n",
    "# \n",
    "# # Silhouette Score\n",
    "# axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "# axes[1].axvline(x=optimal_k_silhouette, color='g', linestyle='--', label=f'Optimal k={optimal_k_silhouette}')\n",
    "# axes[1].set_xlabel('Number of Clusters (k)')\n",
    "# axes[1].set_ylabel('Silhouette Score')\n",
    "# axes[1].set_title('Silhouette Score (higher is better)')\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(True)\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # Print detailed scores for manual inspection\n",
    "# print(\"\\nDetailed scores:\")\n",
    "# print(f\"{'k':<5} {'Inertia':<12} {'Silhouette':<12}\")\n",
    "# print(\"-\" * 30)\n",
    "# for i, k in enumerate(k_range):\n",
    "#     print(f\"{k:<5} {inertias[i]:<12.2f} {silhouette_scores[i]:<12.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a5e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING CODE - COMMENTED OUT\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# \n",
    "# # Convert centroids back to original RGB for plotting\n",
    "# def get_centroids(k):\n",
    "#     kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "#     kmeans_temp.fit(rgb_scaled)\n",
    "#     # inverse-transform to original RGB scale\n",
    "#     return scaler.inverse_transform(kmeans_temp.cluster_centers_)\n",
    "# \n",
    "# # Plot\n",
    "# Ks_to_plot = [10, 11, 12, 13, 14, 15, 16]   # choose any set of k values\n",
    "# n_rows = len(Ks_to_plot)\n",
    "# \n",
    "# fig, axes = plt.subplots(n_rows, 1, figsize=(14, 2*n_rows))\n",
    "# \n",
    "# if n_rows == 1:\n",
    "#     axes = [axes]\n",
    "# \n",
    "# for idx, k in enumerate(Ks_to_plot):\n",
    "#     centroids = get_centroids(k)\n",
    "#     \n",
    "#     # Clip values to RGB range\n",
    "#     centroids = np.clip(centroids, 0, 255).astype(int)\n",
    "#     \n",
    "#     # Build an image strip where each square is a centroid\n",
    "#     color_strip = np.zeros((50, 50*k, 3), dtype=np.uint8)\n",
    "#     for i, color in enumerate(centroids):\n",
    "#         color_strip[:, i*50:(i+1)*50, :] = color\n",
    "#     \n",
    "#     axes[idx].imshow(color_strip)\n",
    "#     axes[idx].set_title(f\"Centroids for k={k}\", fontsize=12)\n",
    "#     axes[idx].axis('off')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec536627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING CODE - COMMENTED OUT\n",
    "# # Perform clustering with optimal k (or choose manually based on the plots above)\n",
    "# # You can use optimal_k_silhouette or choose your own value\n",
    "# N_CLUSTERS = 15\n",
    "# \n",
    "# print(f\"Using k={N_CLUSTERS} clusters\")\n",
    "# \n",
    "# kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)  \n",
    "# df[\"color_cluster\"] = kmeans.fit_predict(rgb_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\"heel_shape_type\", \"toecap_type\", \"archetype\"]\n",
    "df_test.drop(columns=cols_to_remove, inplace=True)\n",
    "\n",
    "family_waist_applicable = df.groupby(\"family\")[\"waist_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"waist_applicable\"] = df_test[\"family\"].map(family_waist_applicable).astype(int)\n",
    "df_test[\"waist_type\"] = df_test.apply(clean_waist_type, axis=1)\n",
    "df_test.drop(columns=[\"waist_applicable\"], inplace=True)\n",
    "\n",
    "family_length_applicable = df.groupby(\"family\")[\"length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"length_applicable\"] = df_test[\"family\"].map(family_length_applicable).astype(int)\n",
    "df_test[\"length_type\"] = df_test.apply(clean_length_type, axis=1)\n",
    "df_test.drop(columns=[\"length_applicable\"], inplace=True)\n",
    "\n",
    "family_silhouette_applicable = df.groupby(\"family\")[\"silhouette_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"silhouette_applicable\"] = df_test[\"family\"].map(family_silhouette_applicable).astype(int)\n",
    "df_test[\"silhouette_type\"] = df_test.apply(clean_silhouette_type, axis=1)\n",
    "df_test.drop(columns=[\"silhouette_applicable\"], inplace=True)\n",
    "\n",
    "family_neck_lapel_applicable = df.groupby(\"family\")[\"neck_lapel_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"neck_lapel_applicable\"] = df_test[\"family\"].map(family_neck_lapel_applicable).astype(int)\n",
    "df_test[\"neck_lapel_type\"] = df_test.apply(clean_neck_lapel_type, axis=1)\n",
    "df_test.drop(columns=[\"neck_lapel_applicable\"], inplace=True)\n",
    "\n",
    "family_sleeve_length_applicable = df.groupby(\"family\")[\"sleeve_length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"sleeve_length_applicable\"] = df_test[\"family\"].map(family_sleeve_length_applicable).astype(int)\n",
    "df_test[\"sleeve_length_type\"] = df_test.apply(clean_sleeve_length_type, axis=1)\n",
    "df_test.drop(columns=[\"sleeve_length_applicable\"], inplace=True)\n",
    "\n",
    "family_woven_structure_applicable = df.groupby(\"family\")[\"woven_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"woven_structure_applicable\"] = df_test[\"family\"].map(family_woven_structure_applicable).astype(int)\n",
    "df_test[\"woven_structure\"] = df_test.apply(clean_woven_structure, axis=1)\n",
    "df_test.drop(columns=[\"woven_structure_applicable\"], inplace=True)\n",
    "\n",
    "family_knit_structure_applicable = df.groupby(\"family\")[\"knit_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"knit_structure_applicable\"] = df_test[\"family\"].map(family_knit_structure_applicable).astype(int)\n",
    "df_test[\"knit_structure\"] = df_test.apply(clean_knit_structure, axis=1)\n",
    "df_test.drop(columns=[\"knit_structure_applicable\"], inplace=True)\n",
    "\n",
    "df_test['is_fall'] = (df_test['id_season'] % 2 == 0).astype(int)\n",
    "\n",
    "df_test[\"print_type\"] = df_test[\"print_type\"].fillna(\"Sin Estampado\")\n",
    "\n",
    "# Color clustering for test data - use the same encoder and kmeans from train\n",
    "print(\"Applying color clustering to test data...\")\n",
    "test_color_names = df_test[\"color_name\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "# Transform test colors using the same label encoder from train\n",
    "# Handle unseen color names (not in train) by assigning to \"UNKNOWN\"\n",
    "test_color_encoded = []\n",
    "for color in test_color_names:\n",
    "    try:\n",
    "        test_color_encoded.append(le_color.transform([color])[0])\n",
    "    except ValueError:\n",
    "        # Color name not seen in training - assign to \"UNKNOWN\" encoding\n",
    "        try:\n",
    "            test_color_encoded.append(le_color.transform([\"UNKNOWN\"])[0])\n",
    "        except ValueError:\n",
    "            # If UNKNOWN doesn't exist, use 0 as fallback\n",
    "            test_color_encoded.append(0)\n",
    "\n",
    "test_color_encoded = np.array(test_color_encoded)\n",
    "test_color_encoded_2d = test_color_encoded.reshape(-1, 1)\n",
    "\n",
    "# Assign test items to color clusters using trained kmeans\n",
    "test_color_clusters = kmeans_color.predict(test_color_encoded_2d)\n",
    "df_test[\"color_cluster\"] = test_color_clusters.astype(int)\n",
    "\n",
    "# Calculate distance to color cluster centroid\n",
    "test_color_dists = kmeans_color.transform(test_color_encoded_2d)\n",
    "df_test[\"color_cluster_dist\"] = test_color_dists.min(axis=1)\n",
    "\n",
    "print(f\"Assigned {len(df_test)} test items to color clusters\")\n",
    "\n",
    "# Drop RGB-related columns (R, G, B, color_rgb) - we're using color_name clustering instead\n",
    "cols_to_drop = [\"color_rgb\"]\n",
    "for col in [\"R\", \"G\", \"B\"]:\n",
    "    if col in df_test.columns:\n",
    "        cols_to_drop.append(col)\n",
    "if len(cols_to_drop) > 1:\n",
    "    df_test.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Dropped RGB columns from test: {cols_to_drop}\")\n",
    "else:\n",
    "    if \"color_rgb\" in df_test.columns:\n",
    "        df_test.drop(columns=[\"color_rgb\"], inplace=True)\n",
    "        print(\"Dropped color_rgb column from test\")\n",
    "\n",
    "# Add seasonality features to test data (week 23 and Black Friday)\n",
    "# Note: Test data doesn't have num_week_iso directly, but we'll add these features\n",
    "# They will be set to 0 for now and can be calculated during prediction time based on actual weeks\n",
    "print(\"\\nAdding seasonality features to test data...\")\n",
    "# Note: For test data, these will be calculated during prediction when we know the actual week\n",
    "# We'll set them to 0 for now (they'll be filled during prediction time in train_model.py)\n",
    "# But let's check if num_week_iso exists in test data\n",
    "if 'num_week_iso' in df_test.columns:\n",
    "    df_test[\"is_week_23\"] = (df_test[\"num_week_iso\"] == 23).astype(int)\n",
    "    df_test[\"is_black_friday\"] = (df_test[\"num_week_iso\"].isin([47, 48])).astype(int)\n",
    "    print(f\"Test - Week 23 occurrences: {df_test['is_week_23'].sum()}\")\n",
    "    print(f\"Test - Black Friday occurrences: {df_test['is_black_friday'].sum()}\")\n",
    "else:\n",
    "    # Will be added during prediction time in train_model.py\n",
    "    df_test[\"is_week_23\"] = 0\n",
    "    df_test[\"is_black_friday\"] = 0\n",
    "    print(\"num_week_iso not in test data - seasonality features will be added during prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23d2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed test data\n",
    "# Note: Test data doesn't have weekly_sales, so we don't need weeks_since_launch for test\n",
    "# weeks_since_launch will be created during prediction time for each week\n",
    "# Save will be done at the end of the notebook\n",
    "# df_test.to_csv('test_processed.csv', index=False)\n",
    "# print(\"Processed test data saved to test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7066fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (95339, 512)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def parse_embedding(x):\n",
    "    \"\"\"Robustly convert string/list/array embedding into np.ndarray.\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return np.array(x)\n",
    "    if x is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        parsed = ast.literal_eval(str(x))\n",
    "        return np.array(parsed)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"embedding_array\"] = df[\"image_embedding\"].apply(parse_embedding)\n",
    "valid = df[\"embedding_array\"].notna()\n",
    "\n",
    "emb_matrix = np.vstack(df.loc[valid, \"embedding_array\"].values)\n",
    "print(\"Embeddings shape:\", emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8791c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 83 PCA features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize embeddings\n",
    "scaler = StandardScaler()\n",
    "emb_scaled = scaler.fit_transform(emb_matrix)\n",
    "\n",
    "# Fit PCA with 83 components\n",
    "n_components = 83\n",
    "pca = PCA(n_components=n_components)\n",
    "emb_pca = pca.fit_transform(emb_scaled)\n",
    "\n",
    "# Add PCA features to dataframe\n",
    "for i in range(n_components):\n",
    "    df.loc[valid, f\"emb_pca_{i+1}\"] = emb_pca[:, i]\n",
    "    df.loc[~valid, f\"emb_pca_{i+1}\"] = 0\n",
    "\n",
    "print(f\"Created {n_components} PCA features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b12bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 22 clusters\n"
     ]
    }
   ],
   "source": [
    "# Clustering with PCA\n",
    "n_clusters = 22 # optimal number with elbow method and silhouette score\n",
    "print(f\"Using {n_clusters} clusters\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "clusters = kmeans.fit_predict(emb_pca)\n",
    "\n",
    "df.loc[valid, \"emb_cluster\"] = clusters\n",
    "df.loc[~valid, \"emb_cluster\"] = -1\n",
    "df[\"emb_cluster\"] = df[\"emb_cluster\"].astype(int)\n",
    "\n",
    "# Distance to centroid\n",
    "dists = kmeans.transform(emb_pca)\n",
    "df.loc[valid, \"emb_dist\"] = dists.min(axis=1)\n",
    "df.loc[~valid, \"emb_dist\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f1f44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cluster-level trend features from train data...\n",
      "Cluster trend features computed for 23 clusters\n",
      "Velocity range: 459.79 to 1469.42\n",
      "\n",
      "Processing test embeddings with trained PCA and KMeans...\n",
      "Assigned 2250 test items to clusters\n",
      "\n",
      "Attaching cluster trend features to train and test data...\n",
      "Train data: velocity_1_3 range 459.79 to 1469.42\n",
      "Test data: velocity_1_3 range 459.79 to 1469.42\n",
      "\n",
      "Cluster trend feature implementation complete!\n",
      "Added feature: velocity_1_3 (average sales per week during first 3 weeks)\n",
      "This feature reflects how similar style groups (clusters) performed historically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janag\\AppData\\Local\\Temp\\ipykernel_15532\\2252065712.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test.loc[test_valid, f\"emb_pca_{i+1}\"] = test_emb_pca[:, i]\n",
      "C:\\Users\\janag\\AppData\\Local\\Temp\\ipykernel_15532\\2252065712.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test.loc[test_valid, \"emb_cluster\"] = test_clusters\n",
      "C:\\Users\\janag\\AppData\\Local\\Temp\\ipykernel_15532\\2252065712.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test.loc[test_valid, \"emb_dist\"] = test_dists.min(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute cluster-level trend features using train data only\n",
    "# velocity_1_3: average sales per week during first 3 weeks of product life\n",
    "print(\"Computing cluster-level trend features from train data...\")\n",
    "\n",
    "# Filter train data for first 3 weeks (weeks_since_launch: 0, 1, 2)\n",
    "train_first_3_weeks = df[df['weeks_since_launch'] < 3].copy()\n",
    "\n",
    "# Compute velocity_1_3 for each cluster\n",
    "cluster_velocity = train_first_3_weeks.groupby('emb_cluster')['weekly_sales'].mean().reset_index()\n",
    "cluster_velocity.columns = ['emb_cluster', 'velocity_1_3']\n",
    "\n",
    "# Add cluster -1 with overall average if it doesn't exist (for invalid embeddings)\n",
    "overall_velocity = train_first_3_weeks['weekly_sales'].mean()\n",
    "if -1 not in cluster_velocity['emb_cluster'].values:\n",
    "    cluster_velocity = pd.concat([\n",
    "        cluster_velocity,\n",
    "        pd.DataFrame([{'emb_cluster': -1, 'velocity_1_3': overall_velocity}])\n",
    "    ], ignore_index=True)\n",
    "\n",
    "print(f\"Cluster trend features computed for {len(cluster_velocity)} clusters\")\n",
    "print(f\"Velocity range: {cluster_velocity['velocity_1_3'].min():.2f} to {cluster_velocity['velocity_1_3'].max():.2f}\")\n",
    "\n",
    "# Step 2: Transform test embeddings using the same PCA and assign to clusters\n",
    "print(\"\\nProcessing test embeddings with trained PCA and KMeans...\")\n",
    "\n",
    "# Parse test embeddings\n",
    "df_test[\"embedding_array\"] = df_test[\"image_embedding\"].apply(parse_embedding)\n",
    "test_valid = df_test[\"embedding_array\"].notna()\n",
    "\n",
    "if test_valid.sum() > 0:\n",
    "    # Stack valid test embeddings\n",
    "    test_emb_matrix = np.vstack(df_test.loc[test_valid, \"embedding_array\"].values)\n",
    "    \n",
    "    # Apply the same scaler and PCA from train\n",
    "    test_emb_scaled = scaler.transform(test_emb_matrix)\n",
    "    test_emb_pca = pca.transform(test_emb_scaled)\n",
    "    \n",
    "    # Assign test items to clusters using trained KMeans\n",
    "    test_clusters = kmeans.predict(test_emb_pca)\n",
    "    \n",
    "    # Add PCA features to test dataframe\n",
    "    for i in range(n_components):\n",
    "        df_test.loc[test_valid, f\"emb_pca_{i+1}\"] = test_emb_pca[:, i]\n",
    "        df_test.loc[~test_valid, f\"emb_pca_{i+1}\"] = 0\n",
    "    \n",
    "    # Add cluster assignments to test dataframe\n",
    "    df_test.loc[test_valid, \"emb_cluster\"] = test_clusters\n",
    "    df_test.loc[~test_valid, \"emb_cluster\"] = -1\n",
    "    df_test[\"emb_cluster\"] = df_test[\"emb_cluster\"].astype(int)\n",
    "    \n",
    "    # Compute distance to centroid for test items\n",
    "    test_dists = kmeans.transform(test_emb_pca)\n",
    "    df_test.loc[test_valid, \"emb_dist\"] = test_dists.min(axis=1)\n",
    "    df_test.loc[~test_valid, \"emb_dist\"] = -1\n",
    "    \n",
    "    print(f\"Assigned {test_valid.sum()} test items to clusters\")\n",
    "else:\n",
    "    print(\"Warning: No valid embeddings found in test data\")\n",
    "    # Add PCA features with zeros\n",
    "    for i in range(n_components):\n",
    "        df_test[f\"emb_pca_{i+1}\"] = 0\n",
    "    df_test[\"emb_cluster\"] = -1\n",
    "    df_test[\"emb_dist\"] = -1\n",
    "\n",
    "# Step 3: Attach cluster trend features to both train and test\n",
    "print(\"\\nAttaching cluster trend features to train and test data...\")\n",
    "\n",
    "# Merge velocity_1_3 to train\n",
    "# If velocity_1_3 already exists in df, drop it first to avoid merge conflicts\n",
    "if 'velocity_1_3' in df.columns:\n",
    "    df = df.drop(columns=['velocity_1_3'])\n",
    "\n",
    "df = df.merge(cluster_velocity, on='emb_cluster', how='left')\n",
    "# Check for any missing clusters (shouldn't happen if all embeddings are valid)\n",
    "if df['velocity_1_3'].isna().any():\n",
    "    overall_velocity = train_first_3_weeks['weekly_sales'].mean()\n",
    "    df['velocity_1_3'] = df['velocity_1_3'].fillna(overall_velocity)\n",
    "    print(f\"Warning: Some clusters missing velocity, filled with overall average: {overall_velocity:.2f}\")\n",
    "\n",
    "# Merge velocity_1_3 to test\n",
    "# If velocity_1_3 already exists in df_test, drop it first to avoid merge conflicts\n",
    "if 'velocity_1_3' in df_test.columns:\n",
    "    df_test = df_test.drop(columns=['velocity_1_3'])\n",
    "\n",
    "df_test = df_test.merge(cluster_velocity, on='emb_cluster', how='left')\n",
    "\n",
    "# Check for any missing clusters (may happen if test has new clusters)\n",
    "if df_test['velocity_1_3'].isna().any():\n",
    "    overall_velocity = train_first_3_weeks['weekly_sales'].mean()\n",
    "    missing_count = df_test['velocity_1_3'].isna().sum()\n",
    "    print(f\"Warning: {missing_count} test items have missing velocity, filling with overall average: {overall_velocity:.2f}\")\n",
    "    df_test['velocity_1_3'] = df_test['velocity_1_3'].fillna(overall_velocity)\n",
    "\n",
    "print(f\"Train data: velocity_1_3 range {df['velocity_1_3'].min():.2f} to {df['velocity_1_3'].max():.2f}\")\n",
    "print(f\"Test data: velocity_1_3 range {df_test['velocity_1_3'].min():.2f} to {df_test['velocity_1_3'].max():.2f}\")\n",
    "\n",
    "print(\"\\nCluster trend feature implementation complete!\")\n",
    "print(f\"Added feature: velocity_1_3 (average sales per week during first 3 weeks)\")\n",
    "print(f\"This feature reflects how similar style groups (clusters) performed historically.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a9a94fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering trend score feature...\n",
      "Computing total sales per product...\n",
      "Top 20% threshold: 14666.80 (1969 products)\n",
      "Bottom 20% threshold: 1112.00 (1973 products)\n",
      "Top centroid computed from 1969 products\n",
      "Bottom centroid computed from 1973 products\n",
      "\n",
      "Computing similarities for train products...\n",
      "Train: trend_score range -1.212 to 1.162\n",
      "Train: Positive trend_score (emerging) count: 50609\n",
      "Train: Negative trend_score (declining) count: 44730\n",
      "\n",
      "Computing similarities for test products...\n",
      "Test: trend_score range -1.161 to 0.986\n",
      "Test: Positive trend_score (emerging) count: 1037\n",
      "Test: Negative trend_score (declining) count: 1213\n",
      "\n",
      "Trend score feature engineering complete!\n",
      "Added features: sim_to_top, sim_to_bottom, trend_score\n",
      "trend_score = sim_to_top - sim_to_bottom\n",
      "  Positive → emerging trend (similar to top performers)\n",
      "  Negative → declining trend (similar to bottom performers)\n"
     ]
    }
   ],
   "source": [
    "# Engineer trend score feature based on similarity to top/bottom performers\n",
    "# This captures whether a product's style is similar to historically successful or unsuccessful products\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Engineering trend score feature...\")\n",
    "\n",
    "# Step 1: Compute total weekly_sales per product (sum across all weeks and seasons)\n",
    "print(\"Computing total sales per product...\")\n",
    "product_sales = df.groupby('ID')['weekly_sales'].sum().reset_index()\n",
    "product_sales.columns = ['ID', 'total_weekly_sales']\n",
    "\n",
    "# Step 2: Identify top 20% and bottom 20% products by total sales\n",
    "top_threshold = product_sales['total_weekly_sales'].quantile(0.8)\n",
    "bottom_threshold = product_sales['total_weekly_sales'].quantile(0.2)\n",
    "\n",
    "top_product_ids = product_sales[product_sales['total_weekly_sales'] >= top_threshold]['ID'].values\n",
    "bottom_product_ids = product_sales[product_sales['total_weekly_sales'] <= bottom_threshold]['ID'].values\n",
    "\n",
    "print(f\"Top 20% threshold: {top_threshold:.2f} ({len(top_product_ids)} products)\")\n",
    "print(f\"Bottom 20% threshold: {bottom_threshold:.2f} ({len(bottom_product_ids)} products)\")\n",
    "\n",
    "# Step 3: Get unique product rows (one per ID) with their PCA embeddings\n",
    "# Take first occurrence of each product to get its embedding\n",
    "train_products = df.groupby('ID').first().reset_index()\n",
    "\n",
    "# Step 4: Compute centroid embeddings for top and bottom performers\n",
    "pca_cols = [f'emb_pca_{i+1}' for i in range(n_components)]\n",
    "\n",
    "top_train_products = train_products[train_products['ID'].isin(top_product_ids)]\n",
    "bottom_train_products = train_products[train_products['ID'].isin(bottom_product_ids)]\n",
    "\n",
    "# Compute mean embedding for top performers\n",
    "top_centroid = top_train_products[pca_cols].mean().values\n",
    "bottom_centroid = bottom_train_products[pca_cols].mean().values\n",
    "\n",
    "print(f\"Top centroid computed from {len(top_train_products)} products\")\n",
    "print(f\"Bottom centroid computed from {len(bottom_train_products)} products\")\n",
    "\n",
    "# Step 5: Compute similarity to centroids for all train products\n",
    "print(\"\\nComputing similarities for train products...\")\n",
    "\n",
    "def compute_similarity_to_centroid(embedding_values, centroid):\n",
    "    \"\"\"Compute cosine similarity between product embedding and centroid.\"\"\"\n",
    "    # Convert to numpy array if it's a pandas Series\n",
    "    if hasattr(embedding_values, 'values'):\n",
    "        embedding_arr = embedding_values.values\n",
    "    else:\n",
    "        embedding_arr = np.array(embedding_values)\n",
    "    \n",
    "    if embedding_arr is None or np.isnan(embedding_arr).any():\n",
    "        return 0.0\n",
    "    # Reshape to 2D arrays for sklearn\n",
    "    embedding_2d = embedding_arr.reshape(1, -1)\n",
    "    centroid_2d = centroid.reshape(1, -1)\n",
    "    return cosine_similarity(embedding_2d, centroid_2d)[0][0]\n",
    "\n",
    "# Get embeddings for train products\n",
    "train_embeddings = train_products[pca_cols]\n",
    "\n",
    "# Compute similarities\n",
    "train_products['sim_to_top'] = train_embeddings.apply(\n",
    "    lambda row: compute_similarity_to_centroid(row, top_centroid), axis=1\n",
    ")\n",
    "train_products['sim_to_bottom'] = train_embeddings.apply(\n",
    "    lambda row: compute_similarity_to_centroid(row, bottom_centroid), axis=1\n",
    ")\n",
    "\n",
    "# Step 6: Merge similarities back to full train dataframe\n",
    "# Drop existing columns if they exist to avoid merge conflicts\n",
    "for col in ['sim_to_top', 'sim_to_bottom', 'trend_score']:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "df = df.merge(train_products[['ID', 'sim_to_top', 'sim_to_bottom']], on='ID', how='left')\n",
    "\n",
    "# Fill any missing values with 0 (shouldn't happen)\n",
    "df['sim_to_top'] = df['sim_to_top'].fillna(0)\n",
    "df['sim_to_bottom'] = df['sim_to_bottom'].fillna(0)\n",
    "\n",
    "# Step 7: Compute trend_score for train\n",
    "df['trend_score'] = df['sim_to_top'] - df['sim_to_bottom']\n",
    "\n",
    "print(f\"Train: trend_score range {df['trend_score'].min():.3f} to {df['trend_score'].max():.3f}\")\n",
    "print(f\"Train: Positive trend_score (emerging) count: {(df['trend_score'] > 0).sum()}\")\n",
    "print(f\"Train: Negative trend_score (declining) count: {(df['trend_score'] < 0).sum()}\")\n",
    "\n",
    "# Step 8: Compute similarities for test products\n",
    "print(\"\\nComputing similarities for test products...\")\n",
    "\n",
    "# Get unique test products (one per ID)\n",
    "test_products = df_test.groupby('ID').first().reset_index()\n",
    "test_embeddings = test_products[pca_cols]\n",
    "\n",
    "# Compute similarities for test\n",
    "test_products['sim_to_top'] = test_embeddings.apply(\n",
    "    lambda row: compute_similarity_to_centroid(row, top_centroid), axis=1\n",
    ")\n",
    "test_products['sim_to_bottom'] = test_embeddings.apply(\n",
    "    lambda row: compute_similarity_to_centroid(row, bottom_centroid), axis=1\n",
    ")\n",
    "\n",
    "# Merge back to full test dataframe\n",
    "# Drop existing columns if they exist to avoid merge conflicts\n",
    "for col in ['sim_to_top', 'sim_to_bottom', 'trend_score']:\n",
    "    if col in df_test.columns:\n",
    "        df_test = df_test.drop(columns=[col])\n",
    "\n",
    "df_test = df_test.merge(test_products[['ID', 'sim_to_top', 'sim_to_bottom']], on='ID', how='left')\n",
    "\n",
    "# Ensure columns exist and fill any missing values with 0\n",
    "if 'sim_to_top' not in df_test.columns:\n",
    "    df_test['sim_to_top'] = 0\n",
    "else:\n",
    "    df_test['sim_to_top'] = df_test['sim_to_top'].fillna(0)\n",
    "\n",
    "if 'sim_to_bottom' not in df_test.columns:\n",
    "    df_test['sim_to_bottom'] = 0\n",
    "else:\n",
    "    df_test['sim_to_bottom'] = df_test['sim_to_bottom'].fillna(0)\n",
    "\n",
    "# Compute trend_score for test\n",
    "df_test['trend_score'] = df_test['sim_to_top'] - df_test['sim_to_bottom']\n",
    "\n",
    "print(f\"Test: trend_score range {df_test['trend_score'].min():.3f} to {df_test['trend_score'].max():.3f}\")\n",
    "print(f\"Test: Positive trend_score (emerging) count: {(df_test['trend_score'] > 0).sum()}\")\n",
    "print(f\"Test: Negative trend_score (declining) count: {(df_test['trend_score'] < 0).sum()}\")\n",
    "\n",
    "print(\"\\nTrend score feature engineering complete!\")\n",
    "print(\"Added features: sim_to_top, sim_to_bottom, trend_score\")\n",
    "print(\"trend_score = sim_to_top - sim_to_bottom\")\n",
    "print(\"  Positive → emerging trend (similar to top performers)\")\n",
    "print(\"  Negative → declining trend (similar to bottom performers)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23b531fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Engineering Advanced Features\n",
      "============================================================\n",
      "\n",
      "1. Cluster-level features...\n",
      "Computing cluster demand last season...\n",
      "Computing cluster season-on-season growth...\n",
      "Computing cluster Y/Y change...\n",
      "  ✓ Cluster velocity 1-6\n",
      "  ✓ Cluster demand last season\n",
      "  ✓ Cluster demand slope\n",
      "  ✓ Cluster popularity\n",
      "  ✓ Cluster season-on-season growth\n",
      "  ✓ Cluster Y/Y change\n",
      "  ✓ Cluster peak week\n",
      "\n",
      "2. Similarity-to-previous-products features...\n",
      "Pre-computing product statistics...\n",
      "Computing similarities for 9843 products in batches of 100...\n",
      "  Processed 1000/9843 products...\n",
      "  Processed 2000/9843 products...\n",
      "  Processed 3000/9843 products...\n",
      "  Processed 4000/9843 products...\n",
      "  Processed 5000/9843 products...\n",
      "  Processed 6000/9843 products...\n",
      "  Processed 7000/9843 products...\n",
      "  Processed 8000/9843 products...\n",
      "  Processed 9000/9843 products...\n",
      "  ✓ Similar product demand mean\n",
      "  ✓ Similar product demand median\n",
      "  ✓ Similar product velocity 1-3\n",
      "\n",
      "3. Family-level trend features...\n",
      "Computing family velocity 1-3 last season...\n",
      "Computing family demand mean last season...\n",
      "Computing family demand trend...\n",
      "  ✓ Family velocity 1-3 last season\n",
      "  ✓ Family demand mean last season\n",
      "  ✓ Family demand trend\n",
      "\n",
      "============================================================\n",
      "Advanced feature engineering complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Advanced feature engineering\n",
    "# Cluster, similarity, and family-level features\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Engineering Advanced Features\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========== CLUSTER-LEVEL FEATURES ==========\n",
    "print(\"\\n1. Cluster-level features...\")\n",
    "\n",
    "# Cluster velocity 1-6 (mean sales per week during first 6 weeks)\n",
    "train_first_6_weeks = df[df['weeks_since_launch'] < 6].copy()\n",
    "cluster_velocity_1_6 = train_first_6_weeks.groupby('emb_cluster')['weekly_sales'].mean().reset_index()\n",
    "cluster_velocity_1_6.columns = ['emb_cluster', 'cluster_velocity_1_6']\n",
    "\n",
    "# Cluster demand last season (mean weekly_sales for each cluster in previous season) - MEMORY OPTIMIZED\n",
    "# For each row, get demand of same cluster in previous season (id_season - 1)\n",
    "print(\"Computing cluster demand last season...\")\n",
    "\n",
    "# Compute means per cluster and season\n",
    "cluster_season_means = df.groupby(['emb_cluster', 'id_season'])['weekly_sales'].mean().reset_index()\n",
    "cluster_season_means.columns = ['emb_cluster', 'id_season', 'cluster_season_mean']\n",
    "\n",
    "# Compute overall cluster means as fallback\n",
    "cluster_overall_means = df.groupby('emb_cluster')['weekly_sales'].mean().reset_index()\n",
    "cluster_overall_means.columns = ['emb_cluster', 'cluster_overall_mean']\n",
    "\n",
    "# Create previous season mapping\n",
    "cluster_season_prev = cluster_season_means.copy()\n",
    "cluster_season_prev['id_season'] = cluster_season_prev['id_season'] + 1\n",
    "cluster_season_prev.columns = ['emb_cluster', 'id_season', 'cluster_demand_last_season']\n",
    "\n",
    "# Merge to get previous season demand\n",
    "cluster_demand_last_season_df = df[['emb_cluster', 'id_season']].drop_duplicates().merge(\n",
    "    cluster_season_prev[['emb_cluster', 'id_season', 'cluster_demand_last_season']],\n",
    "    on=['emb_cluster', 'id_season'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing with cluster overall mean\n",
    "cluster_demand_last_season_df = cluster_demand_last_season_df.merge(\n",
    "    cluster_overall_means,\n",
    "    on='emb_cluster',\n",
    "    how='left'\n",
    ")\n",
    "cluster_demand_last_season_df['cluster_demand_last_season'] = cluster_demand_last_season_df['cluster_demand_last_season'].fillna(\n",
    "    cluster_demand_last_season_df['cluster_overall_mean']\n",
    ")\n",
    "cluster_demand_last_season_df = cluster_demand_last_season_df[['emb_cluster', 'id_season', 'cluster_demand_last_season']]\n",
    "# Drop existing column if it exists to avoid merge conflicts\n",
    "if 'cluster_demand_last_season' in df.columns:\n",
    "    df = df.drop(columns=['cluster_demand_last_season'])\n",
    "\n",
    "df = df.merge(cluster_demand_last_season_df, on=['emb_cluster', 'id_season'], how='left')\n",
    "# Fill missing with cluster overall mean - VECTORIZED (no apply)\n",
    "if 'cluster_demand_last_season' in df.columns:\n",
    "    # Fill missing values using vectorized operations\n",
    "    cluster_overall_mean = df.groupby('emb_cluster')['weekly_sales'].mean().to_dict()\n",
    "    overall_mean_value = df['weekly_sales'].mean()\n",
    "    df['cluster_demand_last_season'] = df['cluster_demand_last_season'].fillna(\n",
    "        df['emb_cluster'].map(cluster_overall_mean).fillna(overall_mean_value)\n",
    "    )\n",
    "else:\n",
    "    # If column doesn't exist after merge, create it\n",
    "    cluster_overall_mean = df.groupby('emb_cluster')['weekly_sales'].mean().to_dict()\n",
    "    overall_mean_value = df['weekly_sales'].mean()\n",
    "    df['cluster_demand_last_season'] = df['emb_cluster'].map(cluster_overall_mean).fillna(overall_mean_value)\n",
    "\n",
    "# Cluster demand slope across weeks (linear regression slope)\n",
    "# For each cluster, compute slope of weekly_sales vs weeks_since_launch\n",
    "cluster_slopes = []\n",
    "for cluster in df['emb_cluster'].unique():\n",
    "    cluster_data = df[df['emb_cluster'] == cluster].copy()\n",
    "    if len(cluster_data) > 1:\n",
    "        weeks = cluster_data['weeks_since_launch'].values\n",
    "        sales = cluster_data['weekly_sales'].values\n",
    "        if len(weeks) > 1 and weeks.std() > 0:\n",
    "            slope, _, _, _, _ = stats.linregress(weeks, sales)\n",
    "        else:\n",
    "            slope = 0\n",
    "    else:\n",
    "        slope = 0\n",
    "    cluster_slopes.append({'emb_cluster': cluster, 'cluster_demand_slope': slope})\n",
    "\n",
    "cluster_slopes_df = pd.DataFrame(cluster_slopes)\n",
    "# Drop existing column if it exists\n",
    "if 'cluster_demand_slope' in df.columns:\n",
    "    df = df.drop(columns=['cluster_demand_slope'])\n",
    "\n",
    "df = df.merge(cluster_slopes_df, on='emb_cluster', how='left')\n",
    "if 'cluster_demand_slope' in df.columns:\n",
    "    df['cluster_demand_slope'] = df['cluster_demand_slope'].fillna(0)\n",
    "else:\n",
    "    df['cluster_demand_slope'] = 0\n",
    "\n",
    "# Cluster popularity (# unique products per cluster)\n",
    "cluster_popularity = df.groupby('emb_cluster')['ID'].nunique().reset_index()\n",
    "cluster_popularity.columns = ['emb_cluster', 'cluster_popularity']\n",
    "# Drop existing column if it exists\n",
    "if 'cluster_popularity' in df.columns:\n",
    "    df = df.drop(columns=['cluster_popularity'])\n",
    "\n",
    "df = df.merge(cluster_popularity, on='emb_cluster', how='left')\n",
    "\n",
    "# Cluster season-on-season growth - MEMORY OPTIMIZED\n",
    "# Compute growth rate between consecutive seasons for each cluster\n",
    "print(\"Computing cluster season-on-season growth...\")\n",
    "\n",
    "# Use the cluster_season_means we already computed\n",
    "cluster_season_means = df.groupby(['emb_cluster', 'id_season'])['weekly_sales'].mean().reset_index()\n",
    "cluster_season_means.columns = ['emb_cluster', 'id_season', 'cluster_season_mean']\n",
    "cluster_season_means = cluster_season_means.sort_values(['emb_cluster', 'id_season'])\n",
    "\n",
    "# Compute previous season mean by shifting within each cluster\n",
    "cluster_season_means['prev_season_mean'] = cluster_season_means.groupby('emb_cluster')['cluster_season_mean'].shift(1)\n",
    "\n",
    "# Compute growth\n",
    "cluster_season_means['cluster_season_growth'] = np.where(\n",
    "    cluster_season_means['prev_season_mean'] > 0,\n",
    "    (cluster_season_means['cluster_season_mean'] - cluster_season_means['prev_season_mean']) / \n",
    "    cluster_season_means['prev_season_mean'],\n",
    "    np.where(cluster_season_means['cluster_season_mean'] > 0, 1.0, 0)\n",
    ")\n",
    "\n",
    "# Fill first season in each cluster with 0 (no previous season)\n",
    "cluster_season_means['cluster_season_growth'] = cluster_season_means['cluster_season_growth'].fillna(0)\n",
    "\n",
    "cluster_growth_df = cluster_season_means[['emb_cluster', 'id_season', 'cluster_season_growth']].copy()\n",
    "# Drop existing column if it exists\n",
    "if 'cluster_season_growth' in df.columns:\n",
    "    df = df.drop(columns=['cluster_season_growth'])\n",
    "\n",
    "df = df.merge(cluster_growth_df, on=['emb_cluster', 'id_season'], how='left')\n",
    "if 'cluster_season_growth' in df.columns:\n",
    "    df['cluster_season_growth'] = df['cluster_season_growth'].fillna(0)\n",
    "else:\n",
    "    df['cluster_season_growth'] = 0\n",
    "\n",
    "# Cluster demand Y/Y change (year-over-year trend) - MEMORY OPTIMIZED\n",
    "# Compare same season across different years\n",
    "print(\"Computing cluster Y/Y change...\")\n",
    "df['season_type'] = (df['id_season'] % 2).astype(int)  # 0 or 1\n",
    "\n",
    "# Compute season_type means per cluster, year, and season_type\n",
    "cluster_yoy_means = df.groupby(['emb_cluster', 'year', 'season_type'])['weekly_sales'].mean().reset_index()\n",
    "cluster_yoy_means.columns = ['emb_cluster', 'year', 'season_type', 'cluster_season_mean']\n",
    "\n",
    "# Merge current year means\n",
    "cluster_yoy_computed = df[['emb_cluster', 'ID', 'id_season', 'year', 'season_type']].copy()\n",
    "cluster_yoy_computed = cluster_yoy_computed.merge(\n",
    "    cluster_yoy_means, \n",
    "    on=['emb_cluster', 'year', 'season_type'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge previous year means\n",
    "cluster_yoy_means_prev = cluster_yoy_means.copy()\n",
    "cluster_yoy_means_prev['year'] = cluster_yoy_means_prev['year'] + 1\n",
    "cluster_yoy_means_prev.columns = ['emb_cluster', 'year', 'season_type', 'cluster_season_mean_prev']\n",
    "cluster_yoy_computed = cluster_yoy_computed.merge(\n",
    "    cluster_yoy_means_prev[['emb_cluster', 'year', 'season_type', 'cluster_season_mean_prev']],\n",
    "    on=['emb_cluster', 'year', 'season_type'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing previous year means with current year mean\n",
    "cluster_yoy_computed['cluster_season_mean_prev'] = cluster_yoy_computed['cluster_season_mean_prev'].fillna(\n",
    "    cluster_yoy_computed['cluster_season_mean']\n",
    ")\n",
    "\n",
    "# Compute Y/Y change\n",
    "cluster_yoy_computed['cluster_yoy_change'] = np.where(\n",
    "    cluster_yoy_computed['cluster_season_mean_prev'] > 0,\n",
    "    (cluster_yoy_computed['cluster_season_mean'] - cluster_yoy_computed['cluster_season_mean_prev']) / \n",
    "    cluster_yoy_computed['cluster_season_mean_prev'],\n",
    "    0\n",
    ")\n",
    "\n",
    "cluster_yoy_df = cluster_yoy_computed[['emb_cluster', 'ID', 'id_season', 'cluster_yoy_change']].copy()\n",
    "\n",
    "# Clean up temporary columns\n",
    "df = df.drop(columns=['season_type'])\n",
    "# Drop existing column if it exists\n",
    "if 'cluster_yoy_change' in df.columns:\n",
    "    df = df.drop(columns=['cluster_yoy_change'])\n",
    "\n",
    "df = df.merge(cluster_yoy_df, on=['emb_cluster', 'ID', 'id_season'], how='left')\n",
    "if 'cluster_yoy_change' in df.columns:\n",
    "    df['cluster_yoy_change'] = df['cluster_yoy_change'].fillna(0)\n",
    "else:\n",
    "    df['cluster_yoy_change'] = 0\n",
    "\n",
    "# Cluster sell-through curve (peak week)\n",
    "# For each cluster, find the week with highest average sales\n",
    "cluster_peak_week = df.groupby(['emb_cluster', 'weeks_since_launch'])['weekly_sales'].mean().reset_index()\n",
    "cluster_peak_week = cluster_peak_week.loc[cluster_peak_week.groupby('emb_cluster')['weekly_sales'].idxmax()]\n",
    "cluster_peak_week = cluster_peak_week[['emb_cluster', 'weeks_since_launch']].copy()\n",
    "cluster_peak_week.columns = ['emb_cluster', 'cluster_peak_week']\n",
    "# Drop existing column if it exists\n",
    "if 'cluster_peak_week' in df.columns:\n",
    "    df = df.drop(columns=['cluster_peak_week'])\n",
    "\n",
    "df = df.merge(cluster_peak_week, on='emb_cluster', how='left')\n",
    "if 'cluster_peak_week' in df.columns:\n",
    "    df['cluster_peak_week'] = df['cluster_peak_week'].fillna(df['weeks_since_launch'].median())\n",
    "else:\n",
    "    df['cluster_peak_week'] = df['weeks_since_launch'].median()\n",
    "\n",
    "# Merge velocity_1_6 to df\n",
    "# Drop existing column if it exists\n",
    "if 'cluster_velocity_1_6' in df.columns:\n",
    "    df = df.drop(columns=['cluster_velocity_1_6'])\n",
    "\n",
    "df = df.merge(cluster_velocity_1_6, on='emb_cluster', how='left')\n",
    "overall_velocity_1_6 = train_first_6_weeks['weekly_sales'].mean()\n",
    "if 'cluster_velocity_1_6' in df.columns:\n",
    "    df['cluster_velocity_1_6'] = df['cluster_velocity_1_6'].fillna(overall_velocity_1_6)\n",
    "else:\n",
    "    df['cluster_velocity_1_6'] = overall_velocity_1_6\n",
    "\n",
    "print(f\"  ✓ Cluster velocity 1-6\")\n",
    "print(f\"  ✓ Cluster demand last season\")\n",
    "print(f\"  ✓ Cluster demand slope\")\n",
    "print(f\"  ✓ Cluster popularity\")\n",
    "print(f\"  ✓ Cluster season-on-season growth\")\n",
    "print(f\"  ✓ Cluster Y/Y change\")\n",
    "print(f\"  ✓ Cluster peak week\")\n",
    "\n",
    "# ========== SIMILARITY-TO-PREVIOUS-PRODUCTS FEATURES ==========\n",
    "print(\"\\n2. Similarity-to-previous-products features...\")\n",
    "\n",
    "# Get unique products with their embeddings (PCA features) - MEMORY OPTIMIZED\n",
    "pca_cols = [f'emb_pca_{i+1}' for i in range(n_components)]\n",
    "train_products_unique = df.groupby('ID').first().reset_index()\n",
    "\n",
    "# Prepare embeddings for similarity computation - use float32 to save memory\n",
    "train_products_embeddings = train_products_unique[pca_cols].fillna(0).astype(np.float32).values\n",
    "\n",
    "# Pre-compute product demand statistics to avoid repeated queries\n",
    "print(\"Pre-computing product statistics...\")\n",
    "product_stats = df.groupby('ID').agg({\n",
    "    'weekly_sales': ['mean', 'median']\n",
    "}).reset_index()\n",
    "product_stats.columns = ['ID', 'product_demand_mean', 'product_demand_median']\n",
    "\n",
    "product_velocity = df[df['weeks_since_launch'] < 3].groupby('ID')['weekly_sales'].mean().reset_index()\n",
    "product_velocity.columns = ['ID', 'product_velocity_1_3']\n",
    "\n",
    "# Get overall stats for fallback\n",
    "overall_mean = df['weekly_sales'].mean()\n",
    "overall_median = df['weekly_sales'].median()\n",
    "overall_velocity = df[df['weeks_since_launch'] < 3]['weekly_sales'].mean()\n",
    "\n",
    "# For each product, find similar products (using KNN on embeddings) - MEMORY OPTIMIZED\n",
    "# Use 10 nearest neighbors, process in batches\n",
    "n_neighbors = min(10, len(train_products_unique) - 1)\n",
    "batch_size = 100  # Process in batches to save memory\n",
    "\n",
    "if n_neighbors > 0 and len(train_products_unique) > 1:\n",
    "    print(f\"Computing similarities for {len(train_products_unique)} products in batches of {batch_size}...\")\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors + 1, metric='cosine', algorithm='brute')\n",
    "    knn.fit(train_products_embeddings)\n",
    "    \n",
    "    # Get similar products for each product in batches\n",
    "    similar_product_features = []\n",
    "    product_ids_array = train_products_unique['ID'].values  # Pre-extract for faster access\n",
    "    \n",
    "    for batch_start in range(0, len(train_products_unique), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(train_products_unique))\n",
    "        batch_embeddings = train_products_embeddings[batch_start:batch_end]\n",
    "        \n",
    "        # Get neighbors for this batch\n",
    "        distances, indices = knn.kneighbors(batch_embeddings)\n",
    "        \n",
    "        for i, idx in enumerate(range(batch_start, batch_end)):\n",
    "            product_id = product_ids_array[idx]\n",
    "            \n",
    "            # Exclude self (first neighbor is itself)\n",
    "            similar_indices = indices[i][1:]\n",
    "            similar_product_ids = product_ids_array[similar_indices]\n",
    "            \n",
    "            # Get demand statistics from pre-computed stats\n",
    "            similar_stats = product_stats[product_stats['ID'].isin(similar_product_ids)]\n",
    "            similar_velocities = product_velocity[product_velocity['ID'].isin(similar_product_ids)]\n",
    "            \n",
    "            if len(similar_stats) > 0:\n",
    "                similar_demand_mean = similar_stats['product_demand_mean'].mean()\n",
    "                similar_demand_median = similar_stats['product_demand_median'].median()\n",
    "                \n",
    "                if len(similar_velocities) > 0:\n",
    "                    similar_velocity_1_3 = similar_velocities['product_velocity_1_3'].mean()\n",
    "                else:\n",
    "                    similar_velocity_1_3 = similar_demand_mean\n",
    "            else:\n",
    "                similar_demand_mean = overall_mean\n",
    "                similar_demand_median = overall_median\n",
    "                similar_velocity_1_3 = overall_velocity\n",
    "            \n",
    "            similar_product_features.append({\n",
    "                'ID': product_id,\n",
    "                'similar_product_demand_mean': similar_demand_mean,\n",
    "                'similar_product_demand_median': similar_demand_median,\n",
    "                'similar_product_velocity_1_3': similar_velocity_1_3\n",
    "            })\n",
    "        \n",
    "        if (batch_start // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Processed {batch_end}/{len(train_products_unique)} products...\")\n",
    "    \n",
    "    similar_features_df = pd.DataFrame(similar_product_features)\n",
    "    \n",
    "    # Merge to full dataframe\n",
    "    # Drop existing columns if they exist\n",
    "    for col in ['similar_product_demand_mean', 'similar_product_demand_median', 'similar_product_velocity_1_3']:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    df = df.merge(similar_features_df, on='ID', how='left')\n",
    "    # Fill missing with overall means\n",
    "    if 'similar_product_demand_mean' in df.columns:\n",
    "        df['similar_product_demand_mean'] = df['similar_product_demand_mean'].fillna(df['weekly_sales'].mean())\n",
    "    else:\n",
    "        df['similar_product_demand_mean'] = df['weekly_sales'].mean()\n",
    "    \n",
    "    if 'similar_product_demand_median' in df.columns:\n",
    "        df['similar_product_demand_median'] = df['similar_product_demand_median'].fillna(df['weekly_sales'].median())\n",
    "    else:\n",
    "        df['similar_product_demand_median'] = df['weekly_sales'].median()\n",
    "    \n",
    "    if 'similar_product_velocity_1_3' in df.columns:\n",
    "        df['similar_product_velocity_1_3'] = df['similar_product_velocity_1_3'].fillna(\n",
    "            df[df['weeks_since_launch'] < 3]['weekly_sales'].mean()\n",
    "        )\n",
    "    else:\n",
    "        df['similar_product_velocity_1_3'] = df[df['weeks_since_launch'] < 3]['weekly_sales'].mean()\n",
    "else:\n",
    "    # Fallback if not enough products\n",
    "    overall_mean = df['weekly_sales'].mean()\n",
    "    overall_median = df['weekly_sales'].median()\n",
    "    overall_velocity = df[df['weeks_since_launch'] < 3]['weekly_sales'].mean()\n",
    "    df['similar_product_demand_mean'] = overall_mean\n",
    "    df['similar_product_demand_median'] = overall_median\n",
    "    df['similar_product_velocity_1_3'] = overall_velocity\n",
    "\n",
    "print(f\"  ✓ Similar product demand mean\")\n",
    "print(f\"  ✓ Similar product demand median\")\n",
    "print(f\"  ✓ Similar product velocity 1-3\")\n",
    "\n",
    "# ========== FAMILY-LEVEL TREND FEATURES ==========\n",
    "print(\"\\n3. Family-level trend features...\")\n",
    "\n",
    "# Family velocity 1-3 last season - MEMORY OPTIMIZED\n",
    "print(\"Computing family velocity 1-3 last season...\")\n",
    "\n",
    "# Compute family velocity for first 3 weeks per family and season\n",
    "family_first_3 = df[df['weeks_since_launch'] < 3]\n",
    "family_season_velocity = family_first_3.groupby(['family', 'id_season'])['weekly_sales'].mean().reset_index()\n",
    "family_season_velocity.columns = ['family', 'id_season', 'family_velocity_1_3']\n",
    "\n",
    "# Compute overall family velocity for first 3 weeks as fallback\n",
    "family_overall_velocity_stats = family_first_3.groupby('family')['weekly_sales'].mean().reset_index()\n",
    "family_overall_velocity_stats.columns = ['family', 'family_overall_velocity']\n",
    "\n",
    "# Create previous season mapping\n",
    "family_velocity_prev = family_season_velocity.copy()\n",
    "family_velocity_prev['id_season'] = family_velocity_prev['id_season'] + 1\n",
    "family_velocity_prev.columns = ['family', 'id_season', 'family_velocity_1_3_last_season']\n",
    "\n",
    "# Merge to get previous season velocity\n",
    "family_velocity_last_season_df = df[['family', 'id_season']].drop_duplicates().merge(\n",
    "    family_velocity_prev[['family', 'id_season', 'family_velocity_1_3_last_season']],\n",
    "    on=['family', 'id_season'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing with family overall velocity\n",
    "family_velocity_last_season_df = family_velocity_last_season_df.merge(\n",
    "    family_overall_velocity_stats,\n",
    "    on='family',\n",
    "    how='left'\n",
    ")\n",
    "family_velocity_last_season_df['family_velocity_1_3_last_season'] = family_velocity_last_season_df['family_velocity_1_3_last_season'].fillna(\n",
    "    family_velocity_last_season_df['family_overall_velocity']\n",
    ")\n",
    "family_velocity_last_season_df = family_velocity_last_season_df[['family', 'id_season', 'family_velocity_1_3_last_season']]\n",
    "# Drop existing column if it exists\n",
    "if 'family_velocity_1_3_last_season' in df.columns:\n",
    "    df = df.drop(columns=['family_velocity_1_3_last_season'])\n",
    "\n",
    "df = df.merge(family_velocity_last_season_df, on=['family', 'id_season'], how='left')\n",
    "# Fill missing with family overall mean for first 3 weeks - VECTORIZED (no apply)\n",
    "if 'family_velocity_1_3_last_season' in df.columns:\n",
    "    # Fill missing values using vectorized operations\n",
    "    family_overall_velocity = df[df['weeks_since_launch'] < 3].groupby('family')['weekly_sales'].mean().to_dict()\n",
    "    overall_mean_value = df['weekly_sales'].mean()\n",
    "    df['family_velocity_1_3_last_season'] = df['family_velocity_1_3_last_season'].fillna(\n",
    "        df['family'].map(family_overall_velocity).fillna(overall_mean_value)\n",
    "    )\n",
    "else:\n",
    "    family_overall_velocity = df[df['weeks_since_launch'] < 3].groupby('family')['weekly_sales'].mean().to_dict()\n",
    "    overall_mean_value = df['weekly_sales'].mean()\n",
    "    df['family_velocity_1_3_last_season'] = df['family'].map(family_overall_velocity).fillna(overall_mean_value)\n",
    "\n",
    "# Family demand mean last season - MEMORY OPTIMIZED\n",
    "print(\"Computing family demand mean last season...\")\n",
    "\n",
    "# Compute family means per season\n",
    "family_season_means = df.groupby(['family', 'id_season'])['weekly_sales'].mean().reset_index()\n",
    "family_season_means.columns = ['family', 'id_season', 'family_season_mean']\n",
    "\n",
    "# Compute overall family means as fallback\n",
    "family_overall_means = df.groupby('family')['weekly_sales'].mean().reset_index()\n",
    "family_overall_means.columns = ['family', 'family_overall_mean']\n",
    "\n",
    "# Create previous season mapping\n",
    "family_season_prev = family_season_means.copy()\n",
    "family_season_prev['id_season'] = family_season_prev['id_season'] + 1\n",
    "family_season_prev.columns = ['family', 'id_season', 'family_demand_mean_last_season']\n",
    "\n",
    "# Merge to get previous season demand\n",
    "family_demand_last_season_df = df[['family', 'id_season']].drop_duplicates().merge(\n",
    "    family_season_prev[['family', 'id_season', 'family_demand_mean_last_season']],\n",
    "    on=['family', 'id_season'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing with family overall mean\n",
    "family_demand_last_season_df = family_demand_last_season_df.merge(\n",
    "    family_overall_means,\n",
    "    on='family',\n",
    "    how='left'\n",
    ")\n",
    "family_demand_last_season_df['family_demand_mean_last_season'] = family_demand_last_season_df['family_demand_mean_last_season'].fillna(\n",
    "    family_demand_last_season_df['family_overall_mean']\n",
    ")\n",
    "family_demand_last_season_df = family_demand_last_season_df[['family', 'id_season', 'family_demand_mean_last_season']]\n",
    "# Drop existing column if it exists\n",
    "if 'family_demand_mean_last_season' in df.columns:\n",
    "    df = df.drop(columns=['family_demand_mean_last_season'])\n",
    "\n",
    "df = df.merge(family_demand_last_season_df, on=['family', 'id_season'], how='left')\n",
    "# Fill missing with family overall mean - VECTORIZED (no apply)\n",
    "if 'family_demand_mean_last_season' in df.columns:\n",
    "    # Fill missing values using vectorized operations\n",
    "    family_overall_mean = df.groupby('family')['weekly_sales'].mean().to_dict()\n",
    "    overall_mean_value = df['weekly_sales'].mean()\n",
    "    df['family_demand_mean_last_season'] = df['family_demand_mean_last_season'].fillna(\n",
    "        df['family'].map(family_overall_mean).fillna(overall_mean_value)\n",
    "    )\n",
    "else:\n",
    "    family_overall_mean = df.groupby('family')['weekly_sales'].mean().to_dict()\n",
    "    overall_mean_value = df['weekly_sales'].mean()\n",
    "    df['family_demand_mean_last_season'] = df['family'].map(family_overall_mean).fillna(overall_mean_value)\n",
    "\n",
    "# Family demand trend (slope over last 4 seasons) - MEMORY OPTIMIZED\n",
    "print(\"Computing family demand trend...\")\n",
    "\n",
    "# Use the family_season_means we already computed\n",
    "family_season_means = df.groupby(['family', 'id_season'])['weekly_sales'].mean().reset_index()\n",
    "family_season_means.columns = ['family', 'id_season', 'family_season_mean']\n",
    "family_season_means = family_season_means.sort_values(['family', 'id_season'])\n",
    "\n",
    "# Compute trend for each family using last 4 seasons\n",
    "family_trends = []\n",
    "for family in family_season_means['family'].unique():\n",
    "    family_seasons = family_season_means[family_season_means['family'] == family].sort_values('id_season')\n",
    "    recent_seasons = family_seasons.tail(4)  # Last 4 seasons\n",
    "    \n",
    "    if len(recent_seasons) > 1:\n",
    "        x = np.arange(len(recent_seasons))\n",
    "        y = recent_seasons['family_season_mean'].values\n",
    "        slope, _, _, _, _ = stats.linregress(x, y)\n",
    "    else:\n",
    "        slope = 0\n",
    "    \n",
    "    family_trends.append({'family': family, 'family_demand_trend': slope})\n",
    "\n",
    "family_trend_map = pd.DataFrame(family_trends)\n",
    "\n",
    "# Merge trend to all family-season combinations\n",
    "family_trend_df = df[['family', 'id_season']].drop_duplicates().merge(\n",
    "    family_trend_map,\n",
    "    on='family',\n",
    "    how='left'\n",
    ")\n",
    "family_trend_df['family_demand_trend'] = family_trend_df['family_demand_trend'].fillna(0)\n",
    "# Drop existing column if it exists\n",
    "if 'family_demand_trend' in df.columns:\n",
    "    df = df.drop(columns=['family_demand_trend'])\n",
    "\n",
    "df = df.merge(family_trend_df, on=['family', 'id_season'], how='left')\n",
    "if 'family_demand_trend' in df.columns:\n",
    "    df['family_demand_trend'] = df['family_demand_trend'].fillna(0)\n",
    "else:\n",
    "    df['family_demand_trend'] = 0\n",
    "\n",
    "print(f\"  ✓ Family velocity 1-3 last season\")\n",
    "print(f\"  ✓ Family demand mean last season\")\n",
    "print(f\"  ✓ Family demand trend\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Advanced feature engineering complete!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5540d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Computing Features for Test Data\n",
      "============================================================\n",
      "\n",
      "1. Adding cluster features to test data...\n",
      "  ✓ All cluster features added\n",
      "\n",
      "2. Computing similarity features for test products...\n",
      "Computing test similarities in batches of 100...\n",
      "  Processed 1000/2250 test products...\n",
      "  Processed 2000/2250 test products...\n",
      "  ✓ Similarity features computed using train data\n",
      "\n",
      "3. Adding family features to test data...\n",
      "  ✓ All family features added\n",
      "\n",
      "============================================================\n",
      "Saving final processed dataframes (excluding embeddings)...\n",
      "✓ train_processed.csv saved successfully!\n",
      "✓ test_processed.csv saved successfully!\n",
      "  Excluded 85 embedding columns for faster processing\n",
      "\n",
      "============================================================\n",
      "All feature engineering complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute features for test data using train-based statistics\n",
    "# Test data doesn't have sales, so we use train data for cluster/family features and similarity\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Computing Features for Test Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========== CLUSTER FEATURES FOR TEST ==========\n",
    "print(\"\\n1. Adding cluster features to test data...\")\n",
    "\n",
    "# Cluster velocity 1-6 (already computed from train, merge by cluster)\n",
    "df_test = df_test.merge(cluster_velocity_1_6, on='emb_cluster', how='left')\n",
    "overall_velocity_1_6 = train_first_6_weeks['weekly_sales'].mean()\n",
    "df_test['cluster_velocity_1_6'] = df_test['cluster_velocity_1_6'].fillna(overall_velocity_1_6)\n",
    "\n",
    "# Cluster demand last season - use last season from train for test clusters\n",
    "# For test, we'll use the most recent season's cluster stats\n",
    "latest_season = df['id_season'].max()\n",
    "cluster_latest_season_demand = df[df['id_season'] == latest_season].groupby('emb_cluster')['weekly_sales'].mean().reset_index()\n",
    "cluster_latest_season_demand.columns = ['emb_cluster', 'cluster_demand_last_season']\n",
    "df_test = df_test.merge(cluster_latest_season_demand, on='emb_cluster', how='left')\n",
    "cluster_overall_mean = df.groupby('emb_cluster')['weekly_sales'].mean().to_dict()\n",
    "df_test['cluster_demand_last_season'] = df_test.apply(\n",
    "    lambda row: cluster_overall_mean.get(row['emb_cluster'], df['weekly_sales'].mean()) \n",
    "    if pd.isna(row['cluster_demand_last_season']) \n",
    "    else row['cluster_demand_last_season'], axis=1\n",
    ")\n",
    "\n",
    "# Cluster demand slope (from train)\n",
    "df_test = df_test.merge(cluster_slopes_df, on='emb_cluster', how='left')\n",
    "df_test['cluster_demand_slope'] = df_test['cluster_demand_slope'].fillna(0)\n",
    "\n",
    "# Cluster popularity (from train)\n",
    "df_test = df_test.merge(cluster_popularity, on='emb_cluster', how='left')\n",
    "df_test['cluster_popularity'] = df_test['cluster_popularity'].fillna(0)\n",
    "\n",
    "# Cluster season-on-season growth - use latest growth from train\n",
    "latest_growth = cluster_growth_df[cluster_growth_df['id_season'] == latest_season].copy()\n",
    "if len(latest_growth) > 0:\n",
    "    latest_growth = latest_growth[['emb_cluster', 'cluster_season_growth']]\n",
    "    df_test = df_test.merge(latest_growth, on='emb_cluster', how='left')\n",
    "else:\n",
    "    df_test['cluster_season_growth'] = 0\n",
    "df_test['cluster_season_growth'] = df_test['cluster_season_growth'].fillna(0)\n",
    "\n",
    "# Cluster Y/Y change - use latest from train\n",
    "latest_yoy = cluster_yoy_df[cluster_yoy_df['id_season'] == latest_season].groupby('emb_cluster')['cluster_yoy_change'].mean().reset_index()\n",
    "df_test = df_test.merge(latest_yoy, on='emb_cluster', how='left')\n",
    "df_test['cluster_yoy_change'] = df_test['cluster_yoy_change'].fillna(0)\n",
    "\n",
    "# Cluster peak week (from train)\n",
    "df_test = df_test.merge(cluster_peak_week, on='emb_cluster', how='left')\n",
    "df_test['cluster_peak_week'] = df_test['cluster_peak_week'].fillna(df['weeks_since_launch'].median() if 'weeks_since_launch' in df_test.columns else 10)\n",
    "\n",
    "print(f\"  ✓ All cluster features added\")\n",
    "\n",
    "# ========== SIMILARITY FEATURES FOR TEST ==========\n",
    "print(\"\\n2. Computing similarity features for test products...\")\n",
    "\n",
    "# Get unique test products\n",
    "test_products_unique = df_test.groupby('ID').first().reset_index()\n",
    "test_products_embeddings = test_products_unique[pca_cols].fillna(0).values\n",
    "\n",
    "# Find similar products from TRAIN data for each test product\n",
    "if len(test_products_unique) > 0 and len(train_products_unique) > 0:\n",
    "    # Use KNN on train embeddings to find similar train products for test products\n",
    "    knn_test = NearestNeighbors(n_neighbors=min(10, len(train_products_unique)), metric='cosine')\n",
    "    knn_test.fit(train_products_embeddings)  # Fit on train embeddings\n",
    "    \n",
    "    test_similar_features = []\n",
    "    \n",
    "    for idx, test_product_row in test_products_unique.iterrows():\n",
    "        test_product_id = test_product_row['ID']\n",
    "        test_product_embedding = test_products_embeddings[idx:idx+1]\n",
    "        distances, indices = knn_test.kneighbors(test_product_embedding)\n",
    "        \n",
    "        # Get similar product IDs from train\n",
    "        similar_indices = indices[0]\n",
    "        similar_product_ids = train_products_unique.iloc[similar_indices]['ID'].values\n",
    "        \n",
    "        # Get demand statistics for similar products from TRAIN data\n",
    "        similar_products_data = df[df['ID'].isin(similar_product_ids)]\n",
    "        \n",
    "        if len(similar_products_data) > 0:\n",
    "            similar_demand_mean = similar_products_data['weekly_sales'].mean()\n",
    "            similar_demand_median = similar_products_data['weekly_sales'].median()\n",
    "            \n",
    "            # Velocity 1-3 for similar products\n",
    "            similar_first_3_weeks = similar_products_data[similar_products_data['weeks_since_launch'] < 3]\n",
    "            if len(similar_first_3_weeks) > 0:\n",
    "                similar_velocity_1_3 = similar_first_3_weeks['weekly_sales'].mean()\n",
    "            else:\n",
    "                similar_velocity_1_3 = similar_demand_mean\n",
    "        else:\n",
    "            similar_demand_mean = df['weekly_sales'].mean()\n",
    "            similar_demand_median = df['weekly_sales'].median()\n",
    "            similar_velocity_1_3 = df[df['weeks_since_launch'] < 3]['weekly_sales'].mean()\n",
    "        \n",
    "        test_similar_features.append({\n",
    "            'ID': test_product_id,\n",
    "            'similar_product_demand_mean': similar_demand_mean,\n",
    "            'similar_product_demand_median': similar_demand_median,\n",
    "            'similar_product_velocity_1_3': similar_velocity_1_3\n",
    "        })\n",
    "    \n",
    "    test_similar_features_df = pd.DataFrame(test_similar_features)\n",
    "    df_test = df_test.merge(test_similar_features_df, on='ID', how='left')\n",
    "    \n",
    "    # Fill missing with overall means from train\n",
    "    df_test['similar_product_demand_mean'] = df_test['similar_product_demand_mean'].fillna(df['weekly_sales'].mean())\n",
    "    df_test['similar_product_demand_median'] = df_test['similar_product_demand_median'].fillna(df['weekly_sales'].median())\n",
    "    df_test['similar_product_velocity_1_3'] = df_test['similar_product_velocity_1_3'].fillna(\n",
    "        df[df['weeks_since_launch'] < 3]['weekly_sales'].mean()\n",
    "    )\n",
    "else:\n",
    "    # Fallback\n",
    "    overall_mean = df['weekly_sales'].mean()\n",
    "    overall_median = df['weekly_sales'].median()\n",
    "    overall_velocity = df[df['weeks_since_launch'] < 3]['weekly_sales'].mean()\n",
    "    df_test['similar_product_demand_mean'] = overall_mean\n",
    "    df_test['similar_product_demand_median'] = overall_median\n",
    "    df_test['similar_product_velocity_1_3'] = overall_velocity\n",
    "\n",
    "print(f\"  ✓ Similarity features computed using train data\")\n",
    "\n",
    "# ========== FAMILY FEATURES FOR TEST ==========\n",
    "print(\"\\n3. Adding family features to test data...\")\n",
    "\n",
    "# Family velocity 1-3 last season - use latest season from train\n",
    "family_latest_velocity = family_velocity_last_season_df[family_velocity_last_season_df['id_season'] == latest_season].copy()\n",
    "if len(family_latest_velocity) > 0:\n",
    "    family_latest_velocity = family_latest_velocity[['family', 'family_velocity_1_3_last_season']]\n",
    "    df_test = df_test.merge(family_latest_velocity, on='family', how='left')\n",
    "else:\n",
    "    df_test['family_velocity_1_3_last_season'] = df_test['family'].map(family_overall_velocity).fillna(df['weekly_sales'].mean())\n",
    "df_test['family_velocity_1_3_last_season'] = df_test['family_velocity_1_3_last_season'].fillna(\n",
    "    df_test['family'].map(family_overall_velocity).fillna(df['weekly_sales'].mean())\n",
    ")\n",
    "\n",
    "# Family demand mean last season - use latest season from train\n",
    "family_latest_demand = family_demand_last_season_df[family_demand_last_season_df['id_season'] == latest_season].copy()\n",
    "if len(family_latest_demand) > 0:\n",
    "    family_latest_demand = family_latest_demand[['family', 'family_demand_mean_last_season']]\n",
    "    df_test = df_test.merge(family_latest_demand, on='family', how='left')\n",
    "else:\n",
    "    df_test['family_demand_mean_last_season'] = df_test['family'].map(family_overall_mean).fillna(df['weekly_sales'].mean())\n",
    "df_test['family_demand_mean_last_season'] = df_test['family_demand_mean_last_season'].fillna(\n",
    "    df_test['family'].map(family_overall_mean).fillna(df['weekly_sales'].mean())\n",
    ")\n",
    "\n",
    "# Family demand trend - use latest trend from train\n",
    "family_latest_trend = family_trend_df[family_trend_df['id_season'] == latest_season].copy()\n",
    "if len(family_latest_trend) > 0:\n",
    "    family_latest_trend = family_latest_trend[['family', 'family_demand_trend']]\n",
    "    df_test = df_test.merge(family_latest_trend, on='family', how='left')\n",
    "else:\n",
    "    df_test['family_demand_trend'] = 0\n",
    "df_test['family_demand_trend'] = df_test['family_demand_trend'].fillna(0)\n",
    "\n",
    "print(f\"  ✓ All family features added\")\n",
    "\n",
    "# Save updated dataframes (excluding embedding columns for faster save)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Saving final processed dataframes (excluding embeddings)...\")\n",
    "\n",
    "# Exclude embedding columns\n",
    "embedding_cols_to_exclude = ['image_embedding', 'embedding_array'] + [f'emb_pca_{i+1}' for i in range(n_components)]\n",
    "train_cols_to_save = [col for col in df.columns if col not in embedding_cols_to_exclude]\n",
    "test_cols_to_save = [col for col in df_test.columns if col not in embedding_cols_to_exclude]\n",
    "\n",
    "df[train_cols_to_save].to_csv('train_processed.csv', index=False)\n",
    "df_test[test_cols_to_save].to_csv('test_processed.csv', index=False)\n",
    "\n",
    "print(\"✓ train_processed.csv saved successfully!\")\n",
    "print(\"✓ test_processed.csv saved successfully!\")\n",
    "print(f\"  Excluded {len(embedding_cols_to_exclude)} embedding columns for faster processing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All feature engineering complete!\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
