{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9d631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('data/train.csv', sep=\";\", header=0)\n",
    "df_test = pd.read_csv('data/test.csv', sep=\";\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5638552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "cols_to_remove = [\"heel_shape_type\", \"toecap_type\", \"archetype\"]\n",
    "df.drop(columns=cols_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2817821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if waist_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null waist_type value\n",
    "family_waist_applicable = df.groupby(\"family\")[\"waist_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"waist_applicable\"] = df[\"family\"].map(family_waist_applicable).astype(int)\n",
    "\n",
    "def clean_waist_type(row):\n",
    "    wt = row[\"waist_type\"]\n",
    "    applicable = row[\"waist_applicable\"]\n",
    "\n",
    "    if pd.notnull(wt):\n",
    "        return wt                           # Real waist type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"          # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"         # Attribute irrelevant for this category\n",
    "\n",
    "df[\"waist_type\"] = df.apply(clean_waist_type, axis=1)\n",
    "df.drop(columns=[\"waist_applicable\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581ec43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if length_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null length_type value\n",
    "family_length_applicable = df.groupby(\"family\")[\"length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"length_applicable\"] = df[\"family\"].map(family_length_applicable).astype(int)\n",
    "\n",
    "def clean_length_type(row):\n",
    "    lt = row[\"length_type\"]\n",
    "    applicable = row[\"length_applicable\"]\n",
    "\n",
    "    if pd.notnull(lt):\n",
    "        return lt                           # Real length type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"          # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"         # Attribute irrelevant for this category\n",
    "\n",
    "df[\"length_type\"] = df.apply(clean_length_type, axis=1)\n",
    "df.drop(columns=[\"length_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308894cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if silhouette_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null silhouette_type value\n",
    "family_silhouette_applicable = df.groupby(\"family\")[\"silhouette_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"silhouette_applicable\"] = df[\"family\"].map(family_silhouette_applicable).astype(int)\n",
    "\n",
    "def clean_silhouette_type(row):\n",
    "    st = row[\"silhouette_type\"]\n",
    "    applicable = row[\"silhouette_applicable\"]\n",
    "\n",
    "    if pd.notnull(st):\n",
    "        return st                          # Real silhouette type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"         # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"        # Attribute irrelevant for this category\n",
    "\n",
    "df[\"silhouette_type\"] = df.apply(clean_silhouette_type, axis=1)\n",
    "df.drop(columns=[\"silhouette_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329e3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if neck_lapel_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null neck_lapel_type value\n",
    "family_neck_lapel_applicable = df.groupby(\"family\")[\"neck_lapel_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"neck_lapel_applicable\"] = df[\"family\"].map(family_neck_lapel_applicable).astype(int)\n",
    "\n",
    "def clean_neck_lapel_type(row):\n",
    "    nlt = row[\"neck_lapel_type\"]\n",
    "    applicable = row[\"neck_lapel_applicable\"]\n",
    "\n",
    "    if pd.notnull(nlt):\n",
    "        return nlt                         # Real neck/lapel type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"         # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"        # Attribute irrelevant for this category\n",
    "\n",
    "df[\"neck_lapel_type\"] = df.apply(clean_neck_lapel_type, axis=1)\n",
    "df.drop(columns=[\"neck_lapel_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c81c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if sleeve_length_type is applicable to each family\n",
    "# A family is applicable if it has at least one non-null sleeve_length_type value\n",
    "family_sleeve_length_applicable = df.groupby(\"family\")[\"sleeve_length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"sleeve_length_applicable\"] = df[\"family\"].map(family_sleeve_length_applicable).astype(int)\n",
    "\n",
    "def clean_sleeve_length_type(row):\n",
    "    slt = row[\"sleeve_length_type\"]\n",
    "    applicable = row[\"sleeve_length_applicable\"]\n",
    "\n",
    "    if pd.notnull(slt):\n",
    "        return slt                         # Real sleeve length type from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"         # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"        # Attribute irrelevant for this category\n",
    "\n",
    "df[\"sleeve_length_type\"] = df.apply(clean_sleeve_length_type, axis=1)\n",
    "df.drop(columns=[\"sleeve_length_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c35aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if woven_structure is applicable to each family\n",
    "# A family is applicable if it has at least one non-null woven_structure value\n",
    "family_woven_structure_applicable = df.groupby(\"family\")[\"woven_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"woven_structure_applicable\"] = df[\"family\"].map(family_woven_structure_applicable).astype(int)\n",
    "\n",
    "def clean_woven_structure(row):\n",
    "    ws = row[\"woven_structure\"]\n",
    "    applicable = row[\"woven_structure_applicable\"]\n",
    "\n",
    "    if pd.notnull(ws):\n",
    "        return ws                         # Real woven structure from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"        # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"       # Attribute irrelevant for this category\n",
    "\n",
    "df[\"woven_structure\"] = df.apply(clean_woven_structure, axis=1)\n",
    "df.drop(columns=[\"woven_structure_applicable\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739804d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if knit_structure is applicable to each family\n",
    "# A family is applicable if it has at least one non-null knit_structure value\n",
    "family_knit_structure_applicable = df.groupby(\"family\")[\"knit_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "\n",
    "df[\"knit_structure_applicable\"] = df[\"family\"].map(family_knit_structure_applicable).astype(int)\n",
    "\n",
    "def clean_knit_structure(row):\n",
    "    ks = row[\"knit_structure\"]\n",
    "    applicable = row[\"knit_structure_applicable\"]\n",
    "\n",
    "    if pd.notnull(ks):\n",
    "        return ks                         # Real knit structure from metadata\n",
    "    else:\n",
    "        if applicable == 1:\n",
    "            return \"MISSING_VALUE\"       # Should exist but missing\n",
    "        else:\n",
    "            return \"NOT_APPLICABLE\"      # Attribute irrelevant for this category\n",
    "\n",
    "df[\"knit_structure\"] = df.apply(clean_knit_structure, axis=1)\n",
    "df.drop(columns=[\"knit_structure_applicable\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af503099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create is_fall boolean column based on id_season\n",
    "# Pattern: even id_season values are fall (1), odd values are not fall (0)\n",
    "# Examples: 89->0, 88->1, 87->0, 86->1\n",
    "df['is_fall'] = (df['id_season'] % 2 == 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87fce06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all missing values in print_type with \"Sin Estampado\"\n",
    "df[\"print_type\"] = df[\"print_type\"].fillna(\"Sin Estampado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a61e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weeks_since_launch column\n",
    "# Group by ID and id_season, then rank by num_week_iso (0 = launch week, 1 = second week, etc.)\n",
    "df = df.sort_values(['ID', 'id_season', 'year', 'num_week_iso'])\n",
    "\n",
    "# Works because the difference in num_week_iso is always 1 (same year) or 51 (different year)\n",
    "# Create weeks_since_launch: rank within each (ID, id_season) group\n",
    "# The smallest num_week_iso gets 0, next gets 1, etc.\n",
    "df['weeks_since_launch'] = (df.groupby(['ID', 'id_season'])['num_week_iso'].rank(method='dense', ascending=True) - 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031206d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Fix for Windows threadpoolctl/OpenBLAS issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Disable threadpoolctl to avoid OpenBLAS inspection error on Windows\n",
    "try:\n",
    "    from sklearn import config_context\n",
    "    # Use config_context to disable threadpoolctl\n",
    "    import sklearn.utils.fixes\n",
    "    # Monkey patch threadpool_limits to be a no-op\n",
    "    original_threadpool_limits = sklearn.utils.fixes.threadpool_limits\n",
    "    \n",
    "    class NoOpContext:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            return False\n",
    "    \n",
    "    def patched_threadpool_limits(*args, **kwargs):\n",
    "        return NoOpContext()\n",
    "    \n",
    "    sklearn.utils.fixes.threadpool_limits = patched_threadpool_limits\n",
    "except Exception:\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "def parse_rgb(x):\n",
    "    # Format is \"255,215,0\" - just split by comma\n",
    "    parts = str(x).split(\",\")\n",
    "    return [float(parts[0]), float(parts[1]), float(parts[2])]\n",
    "\n",
    "df[[\"R\", \"G\", \"B\"]] = df[\"color_rgb\"].apply(lambda x: pd.Series(parse_rgb(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eda6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize RGB values to 0-1 range (divide by 255)\n",
    "df[[\"R\", \"G\", \"B\"]] = df[[\"R\", \"G\", \"B\"]] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95fd1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"color_rgb\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812956c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all negative values for weekly_sales and weekly_demand to 0\n",
    "df.loc[df[\"weekly_sales\"] < 0, \"weekly_sales\"] = 0\n",
    "df.loc[df[\"weekly_demand\"] < 0, \"weekly_demand\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb558804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING CODE - COMMENTED OUT\n",
    "# # Determine optimal number of clusters using multiple methods\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Range of k values to test\n",
    "# k_range = range(2, 21)  # Test k from 2 to 20\n",
    "# inertias = []\n",
    "# silhouette_scores = []\n",
    "# \n",
    "# print(\"Evaluating different numbers of clusters...\")\n",
    "# for k in k_range:\n",
    "#     kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "#     labels = kmeans_temp.fit_predict(rgb_scaled)\n",
    "#     \n",
    "#     inertias.append(kmeans_temp.inertia_)\n",
    "#     silhouette_scores.append(silhouette_score(rgb_scaled, labels))\n",
    "#     \n",
    "#     if k % 5 == 0:\n",
    "#         print(f\"  Completed k={k}...\")\n",
    "# \n",
    "# # Find optimal k based on silhouette score\n",
    "# optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "# \n",
    "# print(f\"\\nOptimal k based on Silhouette Score: {optimal_k_silhouette} (score: {max(silhouette_scores):.3f})\")\n",
    "# \n",
    "# # Plot the results\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# # Elbow Method (Inertia)\n",
    "# axes[0].plot(k_range, inertias, 'bo-')\n",
    "# axes[0].set_xlabel('Number of Clusters (k)')\n",
    "# axes[0].set_ylabel('Within-Cluster Sum of Squares (Inertia)')\n",
    "# axes[0].set_title('Elbow Method')\n",
    "# axes[0].grid(True)\n",
    "# \n",
    "# # Silhouette Score\n",
    "# axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "# axes[1].axvline(x=optimal_k_silhouette, color='g', linestyle='--', label=f'Optimal k={optimal_k_silhouette}')\n",
    "# axes[1].set_xlabel('Number of Clusters (k)')\n",
    "# axes[1].set_ylabel('Silhouette Score')\n",
    "# axes[1].set_title('Silhouette Score (higher is better)')\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(True)\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # Print detailed scores for manual inspection\n",
    "# print(\"\\nDetailed scores:\")\n",
    "# print(f\"{'k':<5} {'Inertia':<12} {'Silhouette':<12}\")\n",
    "# print(\"-\" * 30)\n",
    "# for i, k in enumerate(k_range):\n",
    "#     print(f\"{k:<5} {inertias[i]:<12.2f} {silhouette_scores[i]:<12.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4a5e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING CODE - COMMENTED OUT\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# \n",
    "# # Convert centroids back to original RGB for plotting\n",
    "# def get_centroids(k):\n",
    "#     kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "#     kmeans_temp.fit(rgb_scaled)\n",
    "#     # inverse-transform to original RGB scale\n",
    "#     return scaler.inverse_transform(kmeans_temp.cluster_centers_)\n",
    "# \n",
    "# # Plot\n",
    "# Ks_to_plot = [10, 11, 12, 13, 14, 15, 16]   # choose any set of k values\n",
    "# n_rows = len(Ks_to_plot)\n",
    "# \n",
    "# fig, axes = plt.subplots(n_rows, 1, figsize=(14, 2*n_rows))\n",
    "# \n",
    "# if n_rows == 1:\n",
    "#     axes = [axes]\n",
    "# \n",
    "# for idx, k in enumerate(Ks_to_plot):\n",
    "#     centroids = get_centroids(k)\n",
    "#     \n",
    "#     # Clip values to RGB range\n",
    "#     centroids = np.clip(centroids, 0, 255).astype(int)\n",
    "#     \n",
    "#     # Build an image strip where each square is a centroid\n",
    "#     color_strip = np.zeros((50, 50*k, 3), dtype=np.uint8)\n",
    "#     for i, color in enumerate(centroids):\n",
    "#         color_strip[:, i*50:(i+1)*50, :] = color\n",
    "#     \n",
    "#     axes[idx].imshow(color_strip)\n",
    "#     axes[idx].set_title(f\"Centroids for k={k}\", fontsize=12)\n",
    "#     axes[idx].axis('off')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec536627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING CODE - COMMENTED OUT\n",
    "# # Perform clustering with optimal k (or choose manually based on the plots above)\n",
    "# # You can use optimal_k_silhouette or choose your own value\n",
    "# N_CLUSTERS = 15\n",
    "# \n",
    "# print(f\"Using k={N_CLUSTERS} clusters\")\n",
    "# \n",
    "# kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)  \n",
    "# df[\"color_cluster\"] = kmeans.fit_predict(rgb_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08df7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\"heel_shape_type\", \"toecap_type\", \"archetype\"]\n",
    "df_test.drop(columns=cols_to_remove, inplace=True)\n",
    "\n",
    "family_waist_applicable = df.groupby(\"family\")[\"waist_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"waist_applicable\"] = df_test[\"family\"].map(family_waist_applicable).astype(int)\n",
    "df_test[\"waist_type\"] = df_test.apply(clean_waist_type, axis=1)\n",
    "df_test.drop(columns=[\"waist_applicable\"], inplace=True)\n",
    "\n",
    "family_length_applicable = df.groupby(\"family\")[\"length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"length_applicable\"] = df_test[\"family\"].map(family_length_applicable).astype(int)\n",
    "df_test[\"length_type\"] = df_test.apply(clean_length_type, axis=1)\n",
    "df_test.drop(columns=[\"length_applicable\"], inplace=True)\n",
    "\n",
    "family_silhouette_applicable = df.groupby(\"family\")[\"silhouette_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"silhouette_applicable\"] = df_test[\"family\"].map(family_silhouette_applicable).astype(int)\n",
    "df_test[\"silhouette_type\"] = df_test.apply(clean_silhouette_type, axis=1)\n",
    "df_test.drop(columns=[\"silhouette_applicable\"], inplace=True)\n",
    "\n",
    "family_neck_lapel_applicable = df.groupby(\"family\")[\"neck_lapel_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"neck_lapel_applicable\"] = df_test[\"family\"].map(family_neck_lapel_applicable).astype(int)\n",
    "df_test[\"neck_lapel_type\"] = df_test.apply(clean_neck_lapel_type, axis=1)\n",
    "df_test.drop(columns=[\"neck_lapel_applicable\"], inplace=True)\n",
    "\n",
    "family_sleeve_length_applicable = df.groupby(\"family\")[\"sleeve_length_type\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"sleeve_length_applicable\"] = df_test[\"family\"].map(family_sleeve_length_applicable).astype(int)\n",
    "df_test[\"sleeve_length_type\"] = df_test.apply(clean_sleeve_length_type, axis=1)\n",
    "df_test.drop(columns=[\"sleeve_length_applicable\"], inplace=True)\n",
    "\n",
    "family_woven_structure_applicable = df.groupby(\"family\")[\"woven_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"woven_structure_applicable\"] = df_test[\"family\"].map(family_woven_structure_applicable).astype(int)\n",
    "df_test[\"woven_structure\"] = df_test.apply(clean_woven_structure, axis=1)\n",
    "df_test.drop(columns=[\"woven_structure_applicable\"], inplace=True)\n",
    "\n",
    "family_knit_structure_applicable = df.groupby(\"family\")[\"knit_structure\"].apply(\n",
    "    lambda x: x.notna().any()\n",
    ").to_dict()\n",
    "df_test[\"knit_structure_applicable\"] = df_test[\"family\"].map(family_knit_structure_applicable).astype(int)\n",
    "df_test[\"knit_structure\"] = df_test.apply(clean_knit_structure, axis=1)\n",
    "df_test.drop(columns=[\"knit_structure_applicable\"], inplace=True)\n",
    "\n",
    "df_test['is_fall'] = (df_test['id_season'] % 2 == 0).astype(int)\n",
    "\n",
    "df_test[\"print_type\"] = df_test[\"print_type\"].fillna(\"Sin Estampado\")\n",
    "\n",
    "df_test[[\"R\", \"G\", \"B\"]] = df_test[\"color_rgb\"].apply(lambda x: pd.Series(parse_rgb(x)))\n",
    "df_test[[\"R\", \"G\", \"B\"]] = df_test[[\"R\", \"G\", \"B\"]] / 255.0\n",
    "\n",
    "df_test.drop(columns=[\"color_rgb\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e23d2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test data saved to test_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Save processed test data\n",
    "# Note: Test data doesn't have weekly_sales, so we don't need weeks_since_launch for test\n",
    "# weeks_since_launch will be created during prediction time for each week\n",
    "df_test.to_csv('test_processed.csv', index=False)\n",
    "print(\"Processed test data saved to test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc7066fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (95339, 512)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def parse_embedding(x):\n",
    "    \"\"\"Robustly convert string/list/array embedding into np.ndarray.\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return np.array(x)\n",
    "    if x is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        parsed = ast.literal_eval(str(x))\n",
    "        return np.array(parsed)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"embedding_array\"] = df[\"image_embedding\"].apply(parse_embedding)\n",
    "valid = df[\"embedding_array\"].notna()\n",
    "\n",
    "emb_matrix = np.vstack(df.loc[valid, \"embedding_array\"].values)\n",
    "print(\"Embeddings shape:\", emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd8791c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 83 PCA features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize embeddings\n",
    "scaler = StandardScaler()\n",
    "emb_scaled = scaler.fit_transform(emb_matrix)\n",
    "\n",
    "# Fit PCA with 83 components\n",
    "n_components = 83\n",
    "pca = PCA(n_components=n_components)\n",
    "emb_pca = pca.fit_transform(emb_scaled)\n",
    "\n",
    "# Add PCA features to dataframe\n",
    "for i in range(n_components):\n",
    "    df.loc[valid, f\"emb_pca_{i+1}\"] = emb_pca[:, i]\n",
    "    df.loc[~valid, f\"emb_pca_{i+1}\"] = 0\n",
    "\n",
    "print(f\"Created {n_components} PCA features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b12bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 22 clusters\n"
     ]
    }
   ],
   "source": [
    "# Clustering with PCA\n",
    "n_clusters = 22 # optimal number with elbow method and silhouette score\n",
    "print(f\"Using {n_clusters} clusters\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "clusters = kmeans.fit_predict(emb_pca)\n",
    "\n",
    "df.loc[valid, \"emb_cluster\"] = clusters\n",
    "df.loc[~valid, \"emb_cluster\"] = -1\n",
    "df[\"emb_cluster\"] = df[\"emb_cluster\"].astype(int)\n",
    "\n",
    "# Distance to centroid\n",
    "dists = kmeans.transform(emb_pca)\n",
    "df.loc[valid, \"emb_dist\"] = dists.min(axis=1)\n",
    "df.loc[~valid, \"emb_dist\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f44d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
